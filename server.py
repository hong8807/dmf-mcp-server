"""
DMF Intelligence Server (MCP + ì¹´ì¹´ì˜¤í†¡ ì±„ë„ ì±—ë´‡)
===================================================
ì˜ì•½í’ˆì•ˆì „ë‚˜ë¼ DMF ë°ì´í„°ë¥¼ ì¡°íšŒÂ·ë¶„ì„í•˜ëŠ” í†µí•© ì„œë²„

[1] MCP ì„œë²„: Claude Desktop / PlayMCPì—ì„œ ì‚¬ìš©
[2] ì¹´ì¹´ì˜¤ ì›¹í›… API: ì¹´ì¹´ì˜¤ i ì˜¤í”ˆë¹Œë” Skill ì„œë²„

ë°°í¬: Render.com â†’ í•˜ë‚˜ì˜ ì„œë²„ë¡œ ë‘ ê¸°ëŠ¥ ëª¨ë‘ ì œê³µ
"""

import os
import json
import tempfile
import logging
import re
from datetime import datetime, timedelta
from collections import Counter
from typing import Optional
from contextlib import asynccontextmanager

import requests
import pandas as pd
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
import uvicorn

# MCP (ì¡°ê±´ë¶€ ì„í¬íŠ¸ â€” MCP ì—†ì´ë„ ì¹´ì¹´ì˜¤ ì›¹í›…ë§Œìœ¼ë¡œ ë™ì‘ ê°€ëŠ¥)
try:
    from mcp.server.fastmcp import FastMCP
    MCP_AVAILABLE = True
except ImportError:
    MCP_AVAILABLE = False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë¡œê¹… ì„¤ì •
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("dmf-server")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë°ì´í„° ìºì‹± (ì¹´ì¹´ì˜¤ 5ì´ˆ íƒ€ì„ì•„ì›ƒ ëŒ€ì‘)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import threading

_cache = {
    "df": None,           # ìºì‹±ëœ DataFrame
    "last_updated": None, # ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°
    "loading": False,     # ë¡œë”© ì¤‘ ì—¬ë¶€
    "digest": None        # Geminiìš© ë°ì´í„° ìš”ì•½
}
CACHE_TTL = timedelta(hours=24)  # í•˜ë£¨ 1íšŒ ê°±ì‹ 


def _download_dmf_excel() -> str:
    """ì˜ì•½í’ˆì•ˆì „ë‚˜ë¼ì—ì„œ DMF ì—‘ì…€ ë‹¤ìš´ë¡œë“œ â†’ ì„ì‹œ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
    url = "https://nedrug.mfds.go.kr/pbp/CCBAC03/getExcel"
    logger.info("ğŸ“¥ DMF ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ì¤‘...")
    response = requests.get(url, timeout=120)
    response.raise_for_status()

    tmp = tempfile.NamedTemporaryFile(suffix=".xlsx", delete=False)
    tmp.write(response.content)
    tmp.close()
    logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {tmp.name}")
    return tmp.name


def _get_cached_data() -> pd.DataFrame:
    """ìºì‹±ëœ ë°ì´í„° ë°˜í™˜. ì—†ê±°ë‚˜ ë§Œë£Œë˜ë©´ ìƒˆë¡œ ë‹¤ìš´ë¡œë“œ."""
    now = datetime.now()

    # ìºì‹œê°€ ìœ íš¨í•˜ë©´ ë°”ë¡œ ë°˜í™˜
    if (_cache["df"] is not None and
        _cache["last_updated"] is not None and
        now - _cache["last_updated"] < CACHE_TTL):
        logger.info("âš¡ ìºì‹œ ë°ì´í„° ì‚¬ìš©")
        return _cache["df"]

    # ìºì‹œ ê°±ì‹ 
    logger.info("ğŸ”„ ìºì‹œ ê°±ì‹  ì¤‘...")
    excel_path = _download_dmf_excel()
    try:
        df = _load_and_prepare(excel_path)
        _cache["df"] = df
        _cache["last_updated"] = now
        _cache["digest"] = _build_data_digest(df)
        logger.info(f"âœ… ìºì‹œ ê°±ì‹  ì™„ë£Œ ({len(df)}ê±´)")
        return df
    finally:
        os.unlink(excel_path)


def _preload_cache():
    """ì„œë²„ ì‹œì‘ ì‹œ ë°±ê·¸ë¼ìš´ë“œë¡œ ìºì‹œ ë¯¸ë¦¬ ë¡œë“œ"""
    try:
        _cache["loading"] = True
        _get_cached_data()
    except Exception as e:
        logger.error(f"âŒ ìºì‹œ í”„ë¦¬ë¡œë“œ ì‹¤íŒ¨: {e}")
    finally:
        _cache["loading"] = False


def _load_and_prepare(excel_path: str) -> pd.DataFrame:
    """ì—‘ì…€ ë¡œë“œ + ê¸°ë³¸ ì „ì²˜ë¦¬"""
    df = pd.read_excel(excel_path)

    # NaN ì²˜ë¦¬ (ë¹ˆ ì¹¸ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜)
    text_cols = ['ì„±ë¶„ëª…', 'ì‹ ì²­ì¸', 'ì œì¡°ì†Œëª…', 'ì œì¡°êµ­ê°€', 'ë“±ë¡ë²ˆí˜¸',
                 'ì·¨ì†Œ/ì·¨í•˜êµ¬ë¶„', 'ì—°ê³„ì‹¬ì‚¬ë¬¸ì„œë²ˆí˜¸']
    for col in text_cols:
        if col in df.columns:
            df[col] = df[col].fillna('')

    df['ìµœì´ˆë“±ë¡ì¼ì'] = pd.to_datetime(df['ìµœì´ˆë“±ë¡ì¼ì'], errors='coerce')

    df['is_í—ˆì—¬'] = df['ë“±ë¡ë²ˆí˜¸'].astype(str).str.contains(r'\(', na=False)
    df['ë“±ë¡ìœ í˜•'] = df['is_í—ˆì—¬'].map({True: 'í—ˆì—¬(ë³€ê²½)', False: 'ìµœì´ˆë“±ë¡'})

    df['base_dmf'] = df['ë“±ë¡ë²ˆí˜¸'].astype(str).apply(
        lambda x: x.split('(', 1)[0] if '(' in x else x
    )
    has_linked = (df['ì—°ê³„ì‹¬ì‚¬ë¬¸ì„œë²ˆí˜¸'].astype(str).str.strip() != '')
    linked_bases = set(df.loc[has_linked, 'base_dmf'])
    df['has_ì—°ê³„ì‹¬ì‚¬'] = df['base_dmf'].isin(linked_bases)

    active = df[df['ì·¨ì†Œ/ì·¨í•˜êµ¬ë¶„'] == 'ì •ìƒ'].copy()
    return active


def _build_data_digest(df: pd.DataFrame) -> str:
    """Gemini ì»¨í…ìŠ¤íŠ¸ìš© DMF ë°ì´í„° í†µê³„ ìš”ì•½ ìƒì„± (ìºì‹œ ê°±ì‹  ì‹œ 1íšŒ í˜¸ì¶œ)"""
    today = datetime.today()
    lines = []

    lines.append(f"[DMF ë°ì´í„° ìš”ì•½] ê¸°ì¤€ì¼: {today.strftime('%Y-%m-%d')}")
    lines.append(f"ì´ ì •ìƒ DMF ë“±ë¡ê±´ìˆ˜: {len(df)}ê±´")

    # ìµœì´ˆë“±ë¡ vs í—ˆì—¬
    initial = int((~df['is_í—ˆì—¬']).sum())
    change = int(df['is_í—ˆì—¬'].sum())
    linked = int(df['has_ì—°ê³„ì‹¬ì‚¬'].sum())
    lines.append(f"ìµœì´ˆë“±ë¡: {initial}ê±´ / í—ˆì—¬(ë³€ê²½): {change}ê±´ / ì—°ê³„ì‹¬ì‚¬: {linked}ê±´")

    # ìƒìœ„ ì„±ë¶„ TOP 20
    top_ing = df['ì„±ë¶„ëª…'].value_counts().head(20)
    lines.append("\n[ìƒìœ„ ì„±ë¶„ TOP 20]")
    for name, cnt in top_ing.items():
        lines.append(f"  {name}: {cnt}ê±´")

    # êµ­ê°€ë³„ ë¶„í¬
    country_dist = Counter()
    for c in df['ì œì¡°êµ­ê°€'].dropna():
        for cc in str(c).split('@'):
            country_dist[cc.strip()] += 1
    lines.append("\n[êµ­ê°€ë³„ ë¶„í¬]")
    for country, cnt in country_dist.most_common(20):
        pct = cnt / len(df) * 100
        lines.append(f"  {country}: {cnt}ê±´ ({pct:.1f}%)")

    # ìƒìœ„ ì‹ ì²­ì¸ TOP 20
    top_app = df['ì‹ ì²­ì¸'].value_counts().head(20)
    lines.append("\n[ìƒìœ„ ì‹ ì²­ì¸ TOP 20]")
    for name, cnt in top_app.items():
        if name:
            lines.append(f"  {name}: {cnt}ê±´")

    # ìƒìœ„ ì œì¡°ì†Œ TOP 20
    top_mfr = df['ì œì¡°ì†Œëª…'].value_counts().head(20)
    lines.append("\n[ìƒìœ„ ì œì¡°ì†Œ TOP 20]")
    for name, cnt in top_mfr.items():
        if name:
            lines.append(f"  {name}: {cnt}ê±´")

    # ìµœê·¼ 12ê°œì›” ì›”ë³„ ë“±ë¡ ì¶”ì´
    lines.append("\n[ì›”ë³„ ë“±ë¡ ì¶”ì´ (ìµœê·¼ 12ê°œì›”)]")
    for i in range(12, 0, -1):
        m_end = today.replace(day=1) - timedelta(days=1)
        for _ in range(i - 1):
            m_end = m_end.replace(day=1) - timedelta(days=1)
        m_start = m_end.replace(day=1)
        mask = (df['ìµœì´ˆë“±ë¡ì¼ì'] >= pd.Timestamp(m_start)) & \
               (df['ìµœì´ˆë“±ë¡ì¼ì'] <= pd.Timestamp(m_end))
        cnt = int(mask.sum())
        if cnt > 0:
            lines.append(f"  {m_start.strftime('%Y-%m')}: {cnt}ê±´")

    # ìµœê·¼ 7ì¼ ë“±ë¡
    week_ago = today - timedelta(days=7)
    recent_mask = df['ìµœì´ˆë“±ë¡ì¼ì'] >= pd.Timestamp(week_ago)
    recent_cnt = int(recent_mask.sum())
    lines.append(f"\n[ìµœê·¼ 7ì¼ ì‹ ê·œë“±ë¡]: {recent_cnt}ê±´")

    return "\n".join(lines)


def compare_countries(country_a: str, country_b: str) -> dict:
    """ë‘ êµ­ê°€ DMF ë“±ë¡ í˜„í™© ë¹„êµ"""
    try:
        active = _get_cached_data()
        today = datetime.today()
        three_months_ago = today - timedelta(days=90)

        results = {}
        for country in [country_a, country_b]:
            mask = active['ì œì¡°êµ­ê°€'].astype(str).str.contains(country, case=False, na=False)
            found = active[mask]
            recent = found[found['ìµœì´ˆë“±ë¡ì¼ì'] >= pd.Timestamp(three_months_ago)]

            top_ing = found['ì„±ë¶„ëª…'].value_counts().head(5)
            top_mfr = found['ì œì¡°ì†Œëª…'].value_counts().head(5)

            results[country] = {
                "ì „ì²´_ë“±ë¡ê±´ìˆ˜": len(found),
                "ìµœê·¼3ê°œì›”_ì‹ ê·œ": len(recent),
                "ìµœì´ˆë“±ë¡": int((~found['is_í—ˆì—¬']).sum()),
                "í—ˆì—¬_ë³€ê²½": int(found['is_í—ˆì—¬'].sum()),
                "ì—°ê³„ì‹¬ì‚¬": int(found['has_ì—°ê³„ì‹¬ì‚¬'].sum()),
                "ì£¼ìš”_ì„±ë¶„": [{"ì„±ë¶„ëª…": n, "ê±´ìˆ˜": int(c)} for n, c in top_ing.items()],
                "ì£¼ìš”_ì œì¡°ì†Œ": [{"ì œì¡°ì†Œ": n, "ê±´ìˆ˜": int(c)} for n, c in top_mfr.items()]
            }

        return {
            "ë¹„êµ_êµ­ê°€": [country_a, country_b],
            country_a: results[country_a],
            country_b: results[country_b]
        }
    except Exception as e:
        logger.error(f"êµ­ê°€ ë¹„êµ ì‹¤íŒ¨: {e}")
        raise


def get_top_rankings(category: str, top_n: int = 10, period_months: int = None) -> dict:
    """ì¹´í…Œê³ ë¦¬ë³„ ìƒìœ„ ë­í‚¹ ì¡°íšŒ"""
    try:
        active = _get_cached_data()

        # ê¸°ê°„ í•„í„°
        if period_months:
            cutoff = datetime.today() - timedelta(days=period_months * 30)
            filtered = active[active['ìµœì´ˆë“±ë¡ì¼ì'] >= pd.Timestamp(cutoff)]
            period_label = f"ìµœê·¼ {period_months}ê°œì›”"
        else:
            filtered = active
            period_label = "ì „ì²´"

        col_map = {
            'ingredient': 'ì„±ë¶„ëª…',
            'country': 'ì œì¡°êµ­ê°€',
            'applicant': 'ì‹ ì²­ì¸',
            'manufacturer': 'ì œì¡°ì†Œëª…'
        }

        col = col_map.get(category)
        if not col:
            return {"error": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¹´í…Œê³ ë¦¬: {category}. ê°€ëŠ¥: ingredient, country, applicant, manufacturer"}

        if category == 'country':
            counts = Counter()
            for c in filtered[col].dropna():
                for cc in str(c).split('@'):
                    cc = cc.strip()
                    if cc:
                        counts[cc] += 1
        else:
            counts = Counter()
            for val in filtered[col].dropna():
                val = str(val).strip()
                if val:
                    counts[val] += 1

        rankings = [
            {"ìˆœìœ„": i + 1, "ì´ë¦„": name, "ê±´ìˆ˜": cnt}
            for i, (name, cnt) in enumerate(counts.most_common(top_n))
        ]

        return {
            "ì¹´í…Œê³ ë¦¬": category,
            "ê¸°ê°„": period_label,
            "ì´_ëŒ€ìƒìˆ˜": len(counts),
            "ìƒìœ„_ëª©ë¡": rankings
        }
    except Exception as e:
        logger.error(f"ë­í‚¹ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise


# â”€â”€â”€ ë¶„ì„ í•¨ìˆ˜ë“¤ (JSON dict ë°˜í™˜) â”€â”€â”€

def analyze_weekly_dmf(weeks_ago: int = 1) -> dict:
    """ì£¼ê°„ DMF ë“±ë¡ í˜„í™© ë¶„ì„"""
    try:
        active = _get_cached_data()

        today = datetime.today()
        days_since_monday = today.weekday()
        this_monday = today - timedelta(days=days_since_monday)
        target_monday = this_monday - timedelta(weeks=weeks_ago)
        target_friday = target_monday + timedelta(days=4)

        mask = (active['ìµœì´ˆë“±ë¡ì¼ì'] >= pd.Timestamp(target_monday)) & \
               (active['ìµœì´ˆë“±ë¡ì¼ì'] <= pd.Timestamp(target_friday))
        week_df = active[mask].sort_values('ìµœì´ˆë“±ë¡ì¼ì', ascending=False)

        week_label = f"{target_monday.strftime('%m/%d')}~{target_friday.strftime('%m/%d')}"

        if len(week_df) == 0:
            return {"ê¸°ê°„": week_label, "ë©”ì‹œì§€": "í•´ë‹¹ ì£¼ê°„ ì‹ ê·œ DMF ë“±ë¡ ì—†ìŒ", "ì´ê±´ìˆ˜": 0}

        details = []
        for _, row in week_df.iterrows():
            details.append({
                "ë“±ë¡ì¼": row['ìµœì´ˆë“±ë¡ì¼ì'].strftime('%m/%d'),
                "ë“±ë¡ìœ í˜•": 'í—ˆì—¬' if row['is_í—ˆì—¬'] else 'ìµœì´ˆ',
                "ì„±ë¶„ëª…": str(row.get('ì„±ë¶„ëª…', '')),
                "ì‹ ì²­ì¸": str(row.get('ì‹ ì²­ì¸', '')),
                "ì œì¡°ì†Œ": str(row.get('ì œì¡°ì†Œëª…', ''))[:25],
                "êµ­ê°€": str(row.get('ì œì¡°êµ­ê°€', '')).replace('@', '/'),
                "ì—°ê³„ì‹¬ì‚¬": 'O' if row['has_ì—°ê³„ì‹¬ì‚¬'] else 'X'
            })

        return {
            "ê¸°ê°„": week_label,
            "ì´ê±´ìˆ˜": len(week_df),
            "ìµœì´ˆë“±ë¡": int((~week_df['is_í—ˆì—¬']).sum()),
            "í—ˆì—¬_ë³€ê²½": int(week_df['is_í—ˆì—¬'].sum()),
            "ì—°ê³„ì‹¬ì‚¬_ìˆìŒ": int(week_df['has_ì—°ê³„ì‹¬ì‚¬'].sum()),
            "ìƒì„¸ë‚´ì—­": details
        }
    except Exception as e:
        logger.error(f"ì£¼ê°„ ë¶„ì„ ì‹¤íŒ¨: {e}")
        raise


def analyze_monthly_dmf(months_ago: int = 1) -> dict:
    """ì›”ê°„ DMF ë“±ë¡ í˜„í™© ë¶„ì„"""
    try:
        active = _get_cached_data()

        today = datetime.today()
        target_end = today.replace(day=1) - timedelta(days=1)
        for _ in range(months_ago - 1):
            target_end = target_end.replace(day=1) - timedelta(days=1)
        target_start = target_end.replace(day=1)

        month_label = target_start.strftime('%Yë…„ %mì›”')

        mask = (active['ìµœì´ˆë“±ë¡ì¼ì'] >= pd.Timestamp(target_start)) & \
               (active['ìµœì´ˆë“±ë¡ì¼ì'] <= pd.Timestamp(target_end))
        month_df = active[mask]

        prev_end = target_start - timedelta(days=1)
        prev_start = prev_end.replace(day=1)
        prev_mask = (active['ìµœì´ˆë“±ë¡ì¼ì'] >= pd.Timestamp(prev_start)) & \
                    (active['ìµœì´ˆë“±ë¡ì¼ì'] <= pd.Timestamp(prev_end))
        prev_count = int(active[prev_mask].shape[0])

        if prev_count > 0:
            change_pct = (len(month_df) - prev_count) / prev_count * 100
            change_str = f"+{change_pct:.1f}%" if change_pct >= 0 else f"{change_pct:.1f}%"
        else:
            change_str = "N/A"

        countries = []
        for c in month_df['ì œì¡°êµ­ê°€'].dropna():
            for cc in str(c).split('@'):
                countries.append(cc.strip())
        country_counts = Counter(countries).most_common(10)
        total_c = sum(dict(country_counts).values()) if country_counts else 1
        country_list = [
            {"êµ­ê°€": c, "ê±´ìˆ˜": n, "ë¹„ìœ¨": f"{n/total_c*100:.1f}%"}
            for c, n in country_counts
        ]

        top_applicants = month_df.groupby('ì‹ ì²­ì¸').agg(
            ê±´ìˆ˜=('ë“±ë¡ë²ˆí˜¸', 'count')
        ).sort_values('ê±´ìˆ˜', ascending=False).head(5)
        applicant_list = [
            {"ì‹ ì²­ì¸": name, "ê±´ìˆ˜": int(row['ê±´ìˆ˜'])}
            for name, row in top_applicants.iterrows()
        ]

        return {
            "ê¸°ê°„": month_label,
            "ì´ê±´ìˆ˜": len(month_df),
            "ìµœì´ˆë“±ë¡": int((~month_df['is_í—ˆì—¬']).sum()),
            "í—ˆì—¬_ë³€ê²½": int(month_df['is_í—ˆì—¬'].sum()),
            "ì „ì›”ëŒ€ë¹„_ë³€ë™": change_str,
            "ì „ì›”_ê±´ìˆ˜": prev_count,
            "êµ­ê°€ë³„_ë¶„í¬": country_list,
            "ì£¼ìš”_ì‹ ì²­ì¸_TOP5": applicant_list
        }
    except Exception as e:
        logger.error(f"ì›”ê°„ ë¶„ì„ ì‹¤íŒ¨: {e}")
        raise


def search_ingredient(ingredient: str, linked_filter: str = None) -> dict:
    """
    ì„±ë¶„ëª…ìœ¼ë¡œ DMF ê²€ìƒ‰
    
    Args:
        ingredient: ê²€ìƒ‰ í‚¤ì›Œë“œ (ë¶€ë¶„ ë§¤ì¹­)
        linked_filter: 'linked' = ì—°ê³„ì‹¬ì‚¬ ìˆëŠ” ê²ƒë§Œ, 'unlinked' = ì—†ëŠ” ê²ƒë§Œ, None = ì „ì²´
    """
    try:
        active = _get_cached_data()

        mask = active['ì„±ë¶„ëª…'].astype(str).str.contains(ingredient, case=False, na=False)
        found = active[mask].sort_values('ìµœì´ˆë“±ë¡ì¼ì', ascending=False)

        if len(found) == 0:
            return {"ê²€ìƒ‰ì–´": ingredient, "ë©”ì‹œì§€": f"'{ingredient}' ê´€ë ¨ DMF ë“±ë¡ ì—†ìŒ", "ì´ê±´ìˆ˜": 0}

        found_copy = found.copy()
        found_copy['base_dmf'] = found_copy['ë“±ë¡ë²ˆí˜¸'].astype(str).apply(
            lambda x: x.split('(')[0] if '(' in x else x
        )

        # ì„±ë¶„ëª…ë³„ë¡œ ê·¸ë£¹í•‘ (ë™ì¼ í‚¤ì›Œë“œë¼ë„ ë‹¤ë¥¸ ì„±ë¶„ì€ ë¶„ë¦¬)
        ingredient_groups = []
        total_mfr_count = 0
        total_linked_count = 0

        for ing_name, ing_group in found_copy.groupby('ì„±ë¶„ëª…'):
            # ì´ ì„±ë¶„ì˜ ì œì¡°ì›ë³„ ë¶„ì„
            manufacturers = []
            for base, group in ing_group.groupby('base_dmf'):
                first_row = group[~group['is_í—ˆì—¬']]
                if len(first_row) == 0:
                    first_row = group.iloc[:1]
                first_row = first_row.iloc[0]

                heo_count = int(group['is_í—ˆì—¬'].sum())
                is_linked = bool(first_row['has_ì—°ê³„ì‹¬ì‚¬'])
                status = 'ì •ìƒ' if (group['ì·¨ì†Œ/ì·¨í•˜êµ¬ë¶„'] == 'ì •ìƒ').any() else 'ì·¨ì†Œ/ì·¨í•˜'

                mfr_data = {
                    "base_dmf": base,
                    "ì œì¡°ì†Œ": str(first_row.get('ì œì¡°ì†Œëª…', '')),
                    "êµ­ê°€": str(first_row.get('ì œì¡°êµ­ê°€', '')).replace('@', '/'),
                    "ì‹ ì²­ì¸": str(first_row.get('ì‹ ì²­ì¸', '')),
                    "ë“±ë¡ì¼": first_row['ìµœì´ˆë“±ë¡ì¼ì'].strftime('%Y-%m-%d') if pd.notna(first_row['ìµœì´ˆë“±ë¡ì¼ì']) else '',
                    "í—ˆì—¬_ìˆ˜": heo_count,
                    "ì—°ê³„ì‹¬ì‚¬": is_linked,
                    "ìƒíƒœ": status
                }

                # í•„í„° ì ìš©
                if linked_filter == 'linked' and not is_linked:
                    continue
                if linked_filter == 'unlinked' and is_linked:
                    continue

                manufacturers.append(mfr_data)

            if not manufacturers:
                continue

            linked_count = sum(1 for m in manufacturers if m['ì—°ê³„ì‹¬ì‚¬'])
            total_mfr_count += len(manufacturers)
            total_linked_count += linked_count

            # êµ­ê°€ë³„ ë¶„í¬
            country_dist = Counter()
            for m in manufacturers:
                main_country = m['êµ­ê°€'].split('/')[0]
                country_dist[main_country] += 1

            ingredient_groups.append({
                "ì„±ë¶„ëª…": str(ing_name),
                "ì œì¡°ì›ìˆ˜": len(manufacturers),
                "ì—°ê³„ì‹¬ì‚¬_ìˆ˜": linked_count,
                "êµ­ê°€ë³„_ë¶„í¬": [{"êµ­ê°€": k, "ìˆ˜": v} for k, v in country_dist.most_common()],
                "ì œì¡°ì›_ëª©ë¡": manufacturers
            })

        if not ingredient_groups:
            filter_msg = "ì—°ê³„ì‹¬ì‚¬ ë“±ë¡ëœ" if linked_filter == 'linked' else "ì—°ê³„ì‹¬ì‚¬ ë¯¸ë“±ë¡"
            return {"ê²€ìƒ‰ì–´": ingredient, "ë©”ì‹œì§€": f"'{ingredient}' ì¤‘ {filter_msg} ì œì¡°ì›ì´ ì—†ìŠµë‹ˆë‹¤.", "ì´ê±´ìˆ˜": 0}

        return {
            "ê²€ìƒ‰ì–´": ingredient,
            "í•„í„°": linked_filter,
            "ì„±ë¶„_ì¢…ë¥˜ìˆ˜": len(ingredient_groups),
            "ì´_ì œì¡°ì›ìˆ˜": total_mfr_count,
            "ì´_ì—°ê³„ì‹¬ì‚¬ìˆ˜": total_linked_count,
            "ì„±ë¶„ë³„_í˜„í™©": ingredient_groups
        }
    except Exception as e:
        logger.error(f"ì„±ë¶„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        raise


def search_country(country: str) -> dict:
    """êµ­ê°€ë³„ DMF ê²€ìƒ‰"""
    try:
        active = _get_cached_data()

        mask = active['ì œì¡°êµ­ê°€'].astype(str).str.contains(country, case=False, na=False)
        found = active[mask].sort_values('ìµœì´ˆë“±ë¡ì¼ì', ascending=False)

        if len(found) == 0:
            return {"ê²€ìƒ‰_êµ­ê°€": country, "ë©”ì‹œì§€": f"'{country}' ê´€ë ¨ DMF ì—†ìŒ", "ì´ê±´ìˆ˜": 0}

        three_months_ago = datetime.today() - timedelta(days=90)
        recent = found[found['ìµœì´ˆë“±ë¡ì¼ì'] >= pd.Timestamp(three_months_ago)]

        top_ingredients = found['ì„±ë¶„ëª…'].value_counts().head(10)
        ingredient_list = [
            {"ì„±ë¶„ëª…": name, "ê±´ìˆ˜": int(cnt)}
            for name, cnt in top_ingredients.items()
        ]

        top_mfrs = found['ì œì¡°ì†Œëª…'].value_counts().head(10)
        mfr_list = [
            {"ì œì¡°ì†Œ": name, "ê±´ìˆ˜": int(cnt)}
            for name, cnt in top_mfrs.items()
        ]

        return {
            "ê²€ìƒ‰_êµ­ê°€": country,
            "ì „ì²´_ë“±ë¡ê±´ìˆ˜": len(found),
            "ìµœê·¼3ê°œì›”_ì‹ ê·œ": len(recent),
            "ì£¼ìš”_ì„±ë¶„_TOP10": ingredient_list,
            "ì£¼ìš”_ì œì¡°ì†Œ_TOP10": mfr_list
        }
    except Exception as e:
        logger.error(f"êµ­ê°€ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        raise


def search_applicant(applicant: str, month: int = None) -> dict:
    """ì‹ ì²­ì¸ë³„ DMF ê²€ìƒ‰"""
    try:
        active = _get_cached_data()

        mask = active['ì‹ ì²­ì¸'].astype(str).str.contains(applicant, case=False, na=False)
        found = active[mask].sort_values('ìµœì´ˆë“±ë¡ì¼ì', ascending=False)

        if len(found) == 0:
            return {"ê²€ìƒ‰_ì‹ ì²­ì¸": applicant, "ë©”ì‹œì§€": f"'{applicant}' ê´€ë ¨ DMF ë“±ë¡ ì—†ìŒ", "ì´ê±´ìˆ˜": 0}

        # ì›” í•„í„°
        if month:
            year = datetime.today().year
            found_month = found[
                (found['ìµœì´ˆë“±ë¡ì¼ì'].dt.month == month) &
                (found['ìµœì´ˆë“±ë¡ì¼ì'].dt.year == year)
            ]
            month_label = f"{year}ë…„ {month}ì›”"
        else:
            found_month = found
            month_label = "ì „ì²´"

        # ì„±ë¶„ë³„ í˜„í™©
        ingredient_list = []
        for name, group in found_month.groupby('ì„±ë¶„ëª…'):
            group_copy = group.copy()
            group_copy['base_dmf'] = group_copy['ë“±ë¡ë²ˆí˜¸'].astype(str).apply(
                lambda x: x.split('(')[0] if '(' in x else x
            )
            mfr_count = group_copy['base_dmf'].nunique()

            # ì œì¡°ì†Œ ëª©ë¡
            mfrs = []
            for base, bg in group_copy.groupby('base_dmf'):
                first = bg[~bg['is_í—ˆì—¬']]
                if len(first) == 0:
                    first = bg.iloc[:1]
                first = first.iloc[0]
                mfrs.append({
                    "ì œì¡°ì†Œ": str(first.get('ì œì¡°ì†Œëª…', '')),
                    "êµ­ê°€": str(first.get('ì œì¡°êµ­ê°€', '')).replace('@', '/'),
                    "ë“±ë¡ì¼": first['ìµœì´ˆë“±ë¡ì¼ì'].strftime('%Y-%m-%d') if pd.notna(first['ìµœì´ˆë“±ë¡ì¼ì']) else ''
                })

            ingredient_list.append({
                "ì„±ë¶„ëª…": str(name),
                "ë“±ë¡ê±´ìˆ˜": len(group),
                "ì œì¡°ì›ìˆ˜": mfr_count,
                "ì œì¡°ì›": mfrs
            })

        # ì œì¡°êµ­ê°€ ë¶„í¬
        country_dist = Counter()
        for _, row in found_month.iterrows():
            main_country = str(row['ì œì¡°êµ­ê°€']).split('@')[0]
            country_dist[main_country] += 1
        country_list = [{"êµ­ê°€": k, "ê±´ìˆ˜": v} for k, v in country_dist.most_common()]

        return {
            "ê²€ìƒ‰_ì‹ ì²­ì¸": applicant,
            "ê¸°ê°„": month_label,
            "ì´_ë“±ë¡ê±´ìˆ˜": len(found_month),
            "ì·¨ê¸‰_ì„±ë¶„ìˆ˜": len(ingredient_list),
            "êµ­ê°€ë³„_ë¶„í¬": country_list,
            "ì„±ë¶„ë³„_í˜„í™©": sorted(ingredient_list, key=lambda x: x['ë“±ë¡ê±´ìˆ˜'], reverse=True)
        }
    except Exception as e:
        logger.error(f"ì‹ ì²­ì¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        raise


def search_manufacturer(keyword: str) -> dict:
    """ì œì¡°ì†Œëª…ìœ¼ë¡œ DMF ê²€ìƒ‰"""
    try:
        active = _get_cached_data()

        mask = active['ì œì¡°ì†Œëª…'].astype(str).str.contains(keyword, case=False, na=False)
        found = active[mask].sort_values('ìµœì´ˆë“±ë¡ì¼ì', ascending=False)

        if len(found) == 0:
            return {"ê²€ìƒ‰_ì œì¡°ì†Œ": keyword, "ë©”ì‹œì§€": f"'{keyword}' ê´€ë ¨ ì œì¡°ì†Œ ì—†ìŒ", "ì´ê±´ìˆ˜": 0}

        found_copy = found.copy()
        found_copy['base_dmf'] = found_copy['ë“±ë¡ë²ˆí˜¸'].astype(str).apply(
            lambda x: x.split('(')[0] if '(' in x else x
        )

        # ì„±ë¶„ë³„ í˜„í™©
        ingredient_list = []
        for name, group in found_copy.groupby('ì„±ë¶„ëª…'):
            mfr_count = group['base_dmf'].nunique()
            linked_count = group[group['has_ì—°ê³„ì‹¬ì‚¬']]['base_dmf'].nunique()
            applicants = group['ì‹ ì²­ì¸'].unique().tolist()

            ingredient_list.append({
                "ì„±ë¶„ëª…": str(name),
                "ì œì¡°ì›ìˆ˜": mfr_count,
                "ì—°ê³„ì‹¬ì‚¬_ìˆ˜": linked_count,
                "ì‹ ì²­ì¸": [a for a in applicants if a][:3]
            })

        # êµ­ê°€ ì •ë³´
        country_dist = Counter()
        for _, row in found_copy.drop_duplicates('base_dmf').iterrows():
            main_country = str(row['ì œì¡°êµ­ê°€']).split('@')[0]
            country_dist[main_country] += 1

        return {
            "ê²€ìƒ‰_ì œì¡°ì†Œ": keyword,
            "ì´_ë“±ë¡ê±´ìˆ˜": len(found),
            "ì·¨ê¸‰_ì„±ë¶„ìˆ˜": len(ingredient_list),
            "êµ­ê°€ë³„_ë¶„í¬": [{"êµ­ê°€": k, "ê±´ìˆ˜": v} for k, v in country_dist.most_common()],
            "ì„±ë¶„ë³„_í˜„í™©": sorted(ingredient_list, key=lambda x: x['ì œì¡°ì›ìˆ˜'], reverse=True)
        }
    except Exception as e:
        logger.error(f"ì œì¡°ì†Œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        raise


def search_universal(keyword: str, month: int = None) -> tuple:
    """
    í†µí•© ê²€ìƒ‰: ì„±ë¶„ëª… â†’ ì‹ ì²­ì¸ â†’ ì œì¡°ì†Œëª… ìˆœì„œë¡œ ê²€ìƒ‰
    Returns: (search_type, data) íŠœí”Œ
        search_type: 'ingredient' | 'applicant' | 'manufacturer' | 'none'
    """
    active = _get_cached_data()

    # 1ìˆœìœ„: ì„±ë¶„ëª…
    if active['ì„±ë¶„ëª…'].astype(str).str.contains(keyword, case=False, na=False).any():
        return ('ingredient', None)  # ê¸°ì¡´ search_ingredient ì‚¬ìš©

    # 2ìˆœìœ„: ì‹ ì²­ì¸
    if active['ì‹ ì²­ì¸'].astype(str).str.contains(keyword, case=False, na=False).any():
        return ('applicant', search_applicant(keyword, month))

    # 3ìˆœìœ„: ì œì¡°ì†Œëª…
    if active['ì œì¡°ì†Œëª…'].astype(str).str.contains(keyword, case=False, na=False).any():
        return ('manufacturer', search_manufacturer(keyword))

    return ('none', None)


def search_date_range(start_date, end_date) -> dict:
    """ê¸°ê°„ë³„ DMF ë“±ë¡ í˜„í™© ê²€ìƒ‰"""
    try:
        active = _get_cached_data()

        mask = (active['ìµœì´ˆë“±ë¡ì¼ì'] >= pd.Timestamp(start_date)) & \
               (active['ìµœì´ˆë“±ë¡ì¼ì'] <= pd.Timestamp(end_date))
        found = active[mask].sort_values('ìµœì´ˆë“±ë¡ì¼ì', ascending=False)

        period_label = f"{start_date.strftime('%m/%d')}~{end_date.strftime('%m/%d')}"

        if len(found) == 0:
            return {"ê¸°ê°„": period_label, "ë©”ì‹œì§€": f"{period_label} ê¸°ê°„ ì‹ ê·œ DMF ë“±ë¡ ì—†ìŒ", "ì´ê±´ìˆ˜": 0}

        initial = int((~found['is_í—ˆì—¬']).sum())
        change = int(found['is_í—ˆì—¬'].sum())
        linked = int(found['has_ì—°ê³„ì‹¬ì‚¬'].sum())

        # êµ­ê°€ë³„ ë¶„í¬
        country_dist = Counter()
        for _, row in found.iterrows():
            main_country = str(row['ì œì¡°êµ­ê°€']).split('@')[0]
            country_dist[main_country] += 1
        country_list = [{"êµ­ê°€": k, "ê±´ìˆ˜": v} for k, v in country_dist.most_common()]

        # ì„±ë¶„ë³„ ëª©ë¡
        ingredient_list = []
        for name, group in found.groupby('ì„±ë¶„ëª…'):
            ingredient_list.append({
                "ì„±ë¶„ëª…": str(name),
                "ê±´ìˆ˜": len(group),
                "ì‹ ì²­ì¸": group['ì‹ ì²­ì¸'].iloc[0] if len(group) > 0 else '',
                "ì œì¡°ì†Œ": group['ì œì¡°ì†Œëª…'].iloc[0][:20] if len(group) > 0 else '',
                "êµ­ê°€": str(group['ì œì¡°êµ­ê°€'].iloc[0]).split('@')[0] if len(group) > 0 else ''
            })
        ingredient_list.sort(key=lambda x: x['ê±´ìˆ˜'], reverse=True)

        return {
            "ê¸°ê°„": period_label,
            "ì´ê±´ìˆ˜": len(found),
            "ìµœì´ˆë“±ë¡": initial,
            "í—ˆì—¬": change,
            "ì—°ê³„ì‹¬ì‚¬": linked,
            "êµ­ê°€ë³„_ë¶„í¬": country_list,
            "ì„±ë¶„ë³„_í˜„í™©": ingredient_list
        }
    except Exception as e:
        logger.error(f"ê¸°ê°„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        raise


def generate_chat_summary() -> str:
    """ì¹´ì¹´ì˜¤í†¡ ê³µìœ ìš© ê°„ê²°í•œ ìš”ì•½ ë©”ì‹œì§€"""
    try:
        active = _get_cached_data()

        today = datetime.today()
        days_since_monday = today.weekday()
        this_monday = today - timedelta(days=days_since_monday)
        last_monday = this_monday - timedelta(days=7)
        last_friday = last_monday + timedelta(days=4)

        mask = (active['ìµœì´ˆë“±ë¡ì¼ì'] >= pd.Timestamp(last_monday)) & \
               (active['ìµœì´ˆë“±ë¡ì¼ì'] <= pd.Timestamp(last_friday))
        week_df = active[mask].sort_values('ìµœì´ˆë“±ë¡ì¼ì', ascending=False)

        week_label = f"{last_monday.strftime('%m/%d')}~{last_friday.strftime('%m/%d')}"

        lines = []
        lines.append(f"ğŸ“‹ DMF ì£¼ê°„ í˜„í™© ({week_label})")
        lines.append(f"{'='*28}")

        if len(week_df) == 0:
            lines.append("í•´ë‹¹ ì£¼ê°„ ì‹ ê·œ DMF ë“±ë¡ ì—†ìŒ")
        else:
            initial = int((~week_df['is_í—ˆì—¬']).sum())
            change = int(week_df['is_í—ˆì—¬'].sum())
            linked = int(week_df['has_ì—°ê³„ì‹¬ì‚¬'].sum())

            lines.append(f"ì´ {len(week_df)}ê±´ (ìµœì´ˆ {initial} / í—ˆì—¬ {change})")
            lines.append(f"ì—°ê³„ì‹¬ì‚¬ {linked}ê±´")
            lines.append("")

            for _, row in week_df.iterrows():
                reg_type = "ğŸ”µìµœì´ˆ" if not row['is_í—ˆì—¬'] else "ğŸŸ¡í—ˆì—¬"
                linked_mark = "âœ…" if row['has_ì—°ê³„ì‹¬ì‚¬'] else ""
                country = str(row.get('ì œì¡°êµ­ê°€', '')).replace('@', '/').strip()
                ingredient = str(row.get('ì„±ë¶„ëª…', ''))
                applicant = str(row.get('ì‹ ì²­ì¸', ''))

                lines.append(f"{reg_type} {ingredient}")
                lines.append(f"  {applicant} | {country} {linked_mark}")

            lines.append("")
            lines.append("ì¶œì²˜: ì˜ì•½í’ˆì•ˆì „ë‚˜ë¼ DMF ì‹¬ì‚¬ê²°ê³¼")

        return "\n".join(lines)
    except Exception as e:
        logger.error(f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
        raise


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# [1] MCP ì„œë²„ ì„¤ì •
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if MCP_AVAILABLE:
    mcp = FastMCP(
        "dmf-intelligence",
        instructions="""DMF(Drug Master File) ë“±ë¡ í˜„í™©ì„ ì¡°íšŒÂ·ë¶„ì„í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.
        ì˜ì•½í’ˆì•ˆì „ë‚˜ë¼(nedrug.mfds.go.kr)ì˜ ê³µê°œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ
        ì‹ ê·œ DMF ë“±ë¡, êµ­ê°€ë³„/ì„±ë¶„ë³„ ë¶„ì„, ê²½ìŸ ë™í–¥ ë“±ì„ ì œê³µí•©ë‹ˆë‹¤."""
    )

    @mcp.tool()
    def get_weekly_dmf(weeks_ago: int = 1) -> str:
        """ìµœê·¼ ì£¼ê°„ DMF ë“±ë¡ í˜„í™©ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
        try:
            return json.dumps(analyze_weekly_dmf(weeks_ago), ensure_ascii=False, indent=2)
        except Exception as e:
            return json.dumps({"error": str(e)}, ensure_ascii=False)

    @mcp.tool()
    def get_monthly_dmf_summary(months_ago: int = 1) -> str:
        """ì›”ê°„ DMF ë“±ë¡ í˜„í™© ìš”ì•½ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
        try:
            return json.dumps(analyze_monthly_dmf(months_ago), ensure_ascii=False, indent=2)
        except Exception as e:
            return json.dumps({"error": str(e)}, ensure_ascii=False)

    @mcp.tool()
    def search_dmf_by_ingredient(ingredient: str) -> str:
        """íŠ¹ì • ì„±ë¶„ëª…ìœ¼ë¡œ DMF ë“±ë¡ í˜„í™©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        try:
            return json.dumps(search_ingredient(ingredient), ensure_ascii=False, indent=2)
        except Exception as e:
            return json.dumps({"error": str(e)}, ensure_ascii=False)

    @mcp.tool()
    def search_dmf_by_country(country: str) -> str:
        """íŠ¹ì • êµ­ê°€ì˜ DMF ë“±ë¡ í˜„í™©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        try:
            return json.dumps(search_country(country), ensure_ascii=False, indent=2)
        except Exception as e:
            return json.dumps({"error": str(e)}, ensure_ascii=False)

    @mcp.tool()
    def get_dmf_chat_summary() -> str:
        """ì¹´ì¹´ì˜¤í†¡ ê³µìœ ìš© ê°„ê²°í•œ DMF ìš”ì•½ ë©”ì‹œì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            return generate_chat_summary()
        except Exception as e:
            return f"âŒ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# [2] ì¹´ì¹´ì˜¤ i ì˜¤í”ˆë¹Œë” Skill ì›¹í›… API
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@asynccontextmanager
async def lifespan(app):
    """ì„œë²„ ì‹œì‘ ì‹œ ìºì‹œ í”„ë¦¬ë¡œë“œ"""
    thread = threading.Thread(target=_preload_cache, daemon=True)
    thread.start()
    logger.info("ğŸš€ ë°±ê·¸ë¼ìš´ë“œ ìºì‹œ í”„ë¦¬ë¡œë“œ ì‹œì‘")
    yield

app = FastAPI(title="DMF Intelligence Server", version="2.0", lifespan=lifespan)


def kakao_simple_text(text: str) -> dict:
    """ì¹´ì¹´ì˜¤ ì˜¤í”ˆë¹Œë” simpleText ì‘ë‹µ ìƒì„±"""
    return {
        "version": "2.0",
        "template": {
            "outputs": [
                {"simpleText": {"text": text}}
            ]
        }
    }


def kakao_text_with_buttons(text: str, buttons: list) -> dict:
    """ì¹´ì¹´ì˜¤ ì˜¤í”ˆë¹Œë” í…ìŠ¤íŠ¸ + ë²„íŠ¼ ì‘ë‹µ ìƒì„±"""
    return {
        "version": "2.0",
        "template": {
            "outputs": [
                {
                    "basicCard": {
                        "description": text,
                        "buttons": buttons
                    }
                }
            ]
        }
    }


def kakao_quick_replies(text: str, replies: list) -> dict:
    """ì¹´ì¹´ì˜¤ ì˜¤í”ˆë¹Œë” í…ìŠ¤íŠ¸ + ë°”ë¡œê°€ê¸° ì‘ë‹µ ìƒì„±"""
    return {
        "version": "2.0",
        "template": {
            "outputs": [
                {"simpleText": {"text": text}}
            ],
            "quickReplies": replies
        }
    }


def format_weekly_for_kakao(data: dict) -> str:
    """ì£¼ê°„ ë¶„ì„ ê²°ê³¼ë¥¼ ì¹´ì¹´ì˜¤í†¡ ë©”ì‹œì§€ í˜•íƒœë¡œ í¬ë§·"""
    if data.get("ì´ê±´ìˆ˜", 0) == 0:
        return f"ğŸ“‹ DMF ì£¼ê°„ í˜„í™© ({data['ê¸°ê°„']})\n\n{data.get('ë©”ì‹œì§€', 'ë“±ë¡ ì—†ìŒ')}"

    lines = [
        f"ğŸ“‹ DMF ì£¼ê°„ í˜„í™© ({data['ê¸°ê°„']})",
        f"{'â”€'*24}",
        f"ì´ {data['ì´ê±´ìˆ˜']}ê±´ (ìµœì´ˆ {data['ìµœì´ˆë“±ë¡']} / í—ˆì—¬ {data['í—ˆì—¬_ë³€ê²½']})",
        f"ì—°ê³„ì‹¬ì‚¬ {data['ì—°ê³„ì‹¬ì‚¬_ìˆìŒ']}ê±´",
        ""
    ]

    for item in data.get("ìƒì„¸ë‚´ì—­", [])[:15]:  # ì¹´ì¹´ì˜¤í†¡ ê¸€ììˆ˜ ì œí•œ ê³ ë ¤
        reg_icon = "ğŸ”µ" if item['ë“±ë¡ìœ í˜•'] == 'ìµœì´ˆ' else "ğŸŸ¡"
        linked = " âœ…" if item['ì—°ê³„ì‹¬ì‚¬'] == 'O' else ""
        lines.append(f"{reg_icon} {item['ì„±ë¶„ëª…']}")
        lines.append(f"  {item['ì‹ ì²­ì¸']} | {item['êµ­ê°€']}{linked}")

    if len(data.get("ìƒì„¸ë‚´ì—­", [])) > 15:
        lines.append(f"\n... ì™¸ {len(data['ìƒì„¸ë‚´ì—­']) - 15}ê±´")

    lines.append("\nì¶œì²˜: ì˜ì•½í’ˆì•ˆì „ë‚˜ë¼")
    return "\n".join(lines)


def format_monthly_for_kakao(data: dict) -> str:
    """ì›”ê°„ ë¶„ì„ ê²°ê³¼ë¥¼ ì¹´ì¹´ì˜¤í†¡ ë©”ì‹œì§€ í˜•íƒœë¡œ í¬ë§·"""
    lines = [
        f"ğŸ“Š DMF ì›”ê°„ ë¦¬í¬íŠ¸ ({data['ê¸°ê°„']})",
        f"{'â”€'*24}",
        f"ì´ {data['ì´ê±´ìˆ˜']}ê±´ (ì „ì›” {data['ì „ì›”_ê±´ìˆ˜']}ê±´, {data['ì „ì›”ëŒ€ë¹„_ë³€ë™']})",
        f"  ìµœì´ˆë“±ë¡ {data['ìµœì´ˆë“±ë¡']}ê±´ / í—ˆì—¬ {data['í—ˆì—¬_ë³€ê²½']}ê±´",
        ""
    ]

    if data.get("êµ­ê°€ë³„_ë¶„í¬"):
        lines.append("ğŸŒ êµ­ê°€ë³„ ë¶„í¬:")
        for item in data["êµ­ê°€ë³„_ë¶„í¬"][:5]:
            lines.append(f"  {item['êµ­ê°€']}: {item['ê±´ìˆ˜']}ê±´ ({item['ë¹„ìœ¨']})")

    if data.get("ì£¼ìš”_ì‹ ì²­ì¸_TOP5"):
        lines.append("\nğŸ‘¤ ì£¼ìš” ì‹ ì²­ì¸:")
        for item in data["ì£¼ìš”_ì‹ ì²­ì¸_TOP5"]:
            lines.append(f"  {item['ì‹ ì²­ì¸']}: {item['ê±´ìˆ˜']}ê±´")

    lines.append("\nì¶œì²˜: ì˜ì•½í’ˆì•ˆì „ë‚˜ë¼")
    return "\n".join(lines)


def format_ingredient_for_kakao(data: dict) -> str:
    """ì„±ë¶„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¹´ì¹´ì˜¤í†¡ ë©”ì‹œì§€ í˜•íƒœë¡œ í¬ë§· (ì„±ë¶„ëª…ë³„ ê·¸ë£¹í•‘)"""
    if data.get("ì´ê±´ìˆ˜", 0) == 0 and data.get("ì´_ì œì¡°ì›ìˆ˜", 0) == 0:
        return f"ğŸ” '{data['ê²€ìƒ‰ì–´']}' ê²€ìƒ‰ ê²°ê³¼\n\n{data.get('ë©”ì‹œì§€', 'ë“±ë¡ ì—†ìŒ')}"

    linked_filter = data.get("í•„í„°")
    filter_label = ""
    if linked_filter == 'linked':
        filter_label = " [ì—°ê³„ì‹¬ì‚¬ âœ…]"
    elif linked_filter == 'unlinked':
        filter_label = " [ë¯¸ì—°ê³„]"

    lines = [
        f"ğŸ” '{data['ê²€ìƒ‰ì–´']}' DMF í˜„í™©{filter_label}",
        f"{'â”€'*24}",
        f"ğŸ“‹ ì„±ë¶„ {data['ì„±ë¶„_ì¢…ë¥˜ìˆ˜']}ì¢… | ì œì¡°ì› {data['ì´_ì œì¡°ì›ìˆ˜']}ê°œì‚¬ | ì—°ê³„ {data['ì´_ì—°ê³„ì‹¬ì‚¬ìˆ˜']}ê°œ",
    ]

    # ì„±ë¶„ë³„ ìƒì„¸
    for ig in data.get("ì„±ë¶„ë³„_í˜„í™©", []):
        lines.append(f"\n{'â”'*24}")
        lines.append(f"ğŸ’Š {ig['ì„±ë¶„ëª…']}")

        # êµ­ê°€ë³„ ë¶„í¬
        dist = ig.get("êµ­ê°€ë³„_ë¶„í¬", [])
        if dist:
            dist_str = " | ".join([f"{c['êµ­ê°€']} {c['ìˆ˜']}" for c in dist[:4]])
            lines.append(f"   ğŸŒ {dist_str}")

        lines.append(f"   ì œì¡°ì› {ig['ì œì¡°ì›ìˆ˜']}ê°œ (ì—°ê³„ {ig['ì—°ê³„ì‹¬ì‚¬_ìˆ˜']}ê°œ)")

        # ì œì¡°ì› ëª©ë¡ (ì „ì²´ í‘œì‹œ, 1ì¤„ë¡œ ì••ì¶•)
        for m in ig.get("ì œì¡°ì›_ëª©ë¡", []):
            linked_mark = "âœ…" if m['ì—°ê³„ì‹¬ì‚¬'] else "â¬œ"
            status_mark = "âŒ" if m['ìƒíƒœ'] != 'ì •ìƒ' else ""
            heo = f"+{m['í—ˆì—¬_ìˆ˜']}í—ˆì—¬" if m['í—ˆì—¬_ìˆ˜'] > 0 else ""
            country = m['êµ­ê°€'].split('/')[0]  # ì²« ë²ˆì§¸ êµ­ê°€ë§Œ
            lines.append(f"  {linked_mark} {m['ì œì¡°ì†Œ'][:20]} ({country})")
            lines.append(f"     {m['ì‹ ì²­ì¸'][:12]} {heo}{status_mark}")

    lines.append(f"\n{'â”€'*24}")
    lines.append("ì¶œì²˜: ì˜ì•½í’ˆì•ˆì „ë‚˜ë¼")
    return "\n".join(lines)


def format_country_for_kakao(data: dict) -> str:
    """êµ­ê°€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¹´ì¹´ì˜¤í†¡ ë©”ì‹œì§€ í˜•íƒœë¡œ í¬ë§·"""
    if data.get("ì „ì²´_ë“±ë¡ê±´ìˆ˜", 0) == 0:
        return f"ğŸŒ '{data['ê²€ìƒ‰_êµ­ê°€']}' ê²€ìƒ‰ ê²°ê³¼\n\n{data.get('ë©”ì‹œì§€', 'ë“±ë¡ ì—†ìŒ')}"

    lines = [
        f"ğŸŒ {data['ê²€ìƒ‰_êµ­ê°€']} DMF í˜„í™©",
        f"{'â”€'*24}",
        f"ì „ì²´ {data['ì „ì²´_ë“±ë¡ê±´ìˆ˜']}ê±´ (ìµœê·¼3ê°œì›” {data['ìµœê·¼3ê°œì›”_ì‹ ê·œ']}ê±´)",
        ""
    ]

    if data.get("ì£¼ìš”_ì„±ë¶„_TOP10"):
        lines.append("ğŸ’Š ì£¼ìš” ì„±ë¶„:")
        for item in data["ì£¼ìš”_ì„±ë¶„_TOP10"][:7]:
            lines.append(f"  {item['ì„±ë¶„ëª…']}: {item['ê±´ìˆ˜']}ê±´")

    if data.get("ì£¼ìš”_ì œì¡°ì†Œ_TOP10"):
        lines.append("\nğŸ­ ì£¼ìš” ì œì¡°ì†Œ:")
        for item in data["ì£¼ìš”_ì œì¡°ì†Œ_TOP10"][:5]:
            lines.append(f"  {item['ì œì¡°ì†Œ'][:25]}: {item['ê±´ìˆ˜']}ê±´")

    lines.append("\nì¶œì²˜: ì˜ì•½í’ˆì•ˆì „ë‚˜ë¼")
    return "\n".join(lines)


def format_applicant_for_kakao(data: dict) -> str:
    """ì‹ ì²­ì¸ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¹´ì¹´ì˜¤í†¡ ë©”ì‹œì§€ í˜•íƒœë¡œ í¬ë§·"""
    if data.get("ì´_ë“±ë¡ê±´ìˆ˜", 0) == 0:
        return f"ğŸ‘¤ '{data['ê²€ìƒ‰_ì‹ ì²­ì¸']}' ê²€ìƒ‰ ê²°ê³¼\n\n{data.get('ë©”ì‹œì§€', 'ë“±ë¡ ì—†ìŒ')}"

    lines = [
        f"ğŸ‘¤ '{data['ê²€ìƒ‰_ì‹ ì²­ì¸']}' DMF í˜„í™©",
        f"   ({data['ê¸°ê°„']})",
        f"{'â”€'*24}",
        f"ğŸ“‹ ì´ {data['ì´_ë“±ë¡ê±´ìˆ˜']}ê±´ | ì·¨ê¸‰ ì„±ë¶„ {data['ì·¨ê¸‰_ì„±ë¶„ìˆ˜']}ì¢…",
    ]

    # êµ­ê°€ë³„ ë¶„í¬
    country_dist = data.get("êµ­ê°€ë³„_ë¶„í¬", [])
    if country_dist:
        dist_str = " | ".join([f"{c['êµ­ê°€']} {c['ê±´ìˆ˜']}" for c in country_dist[:4]])
        lines.append(f"ğŸŒ {dist_str}")

    lines.append(f"{'â”€'*24}")

    # ì„±ë¶„ë³„ í˜„í™©
    ingredients = data.get("ì„±ë¶„ë³„_í˜„í™©", [])
    if ingredients:
        for item in ingredients[:8]:
            lines.append(f"\nğŸ’Š {item['ì„±ë¶„ëª…'][:20]}")
            lines.append(f"   ì œì¡°ì› {item['ì œì¡°ì›ìˆ˜']}ê°œì‚¬")
            for mfr in item.get('ì œì¡°ì›', [])[:3]:
                lines.append(f"   â–ª {mfr['ì œì¡°ì†Œ'][:22]} ({mfr['êµ­ê°€']})")

    if len(ingredients) > 8:
        lines.append(f"\n... ì™¸ {len(ingredients) - 8}ê°œ ì„±ë¶„")

    lines.append("\nì¶œì²˜: ì˜ì•½í’ˆì•ˆì „ë‚˜ë¼")
    return "\n".join(lines)


def format_manufacturer_for_kakao(data: dict) -> str:
    """ì œì¡°ì†Œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¹´ì¹´ì˜¤í†¡ ë©”ì‹œì§€ í˜•íƒœë¡œ í¬ë§·"""
    if data.get("ì´ê±´ìˆ˜", 0) == 0:
        return f"ğŸ­ '{data['ê²€ìƒ‰_ì œì¡°ì†Œ']}' ê²€ìƒ‰ ê²°ê³¼\n\n{data.get('ë©”ì‹œì§€', 'ë“±ë¡ ì—†ìŒ')}"

    lines = [
        f"ğŸ­ '{data['ê²€ìƒ‰_ì œì¡°ì†Œ']}' ì œì¡°ì†Œ í˜„í™©",
        f"{'â”€'*24}",
        f"ğŸ“‹ ì´ {data['ì´_ë“±ë¡ê±´ìˆ˜']}ê±´ | ì·¨ê¸‰ ì„±ë¶„ {data['ì·¨ê¸‰_ì„±ë¶„ìˆ˜']}ì¢…",
    ]

    country_dist = data.get("êµ­ê°€ë³„_ë¶„í¬", [])
    if country_dist:
        dist_str = " | ".join([f"{c['êµ­ê°€']} {c['ê±´ìˆ˜']}" for c in country_dist[:4]])
        lines.append(f"ğŸŒ {dist_str}")

    lines.append(f"{'â”€'*24}")

    ingredients = data.get("ì„±ë¶„ë³„_í˜„í™©", [])
    if ingredients:
        for item in ingredients[:12]:
            linked_mark = f"âœ…{item['ì—°ê³„ì‹¬ì‚¬_ìˆ˜']}" if item['ì—°ê³„ì‹¬ì‚¬_ìˆ˜'] > 0 else "â¬œ0"
            apps = ", ".join(item.get('ì‹ ì²­ì¸', [])[:2])
            lines.append(f"ğŸ’Š {item['ì„±ë¶„ëª…'][:20]}")
            lines.append(f"   ì œì¡°ì› {item['ì œì¡°ì›ìˆ˜']}ê°œ | ì—°ê³„ {linked_mark} | {apps[:15]}")

    if len(ingredients) > 12:
        lines.append(f"\n... ì™¸ {len(ingredients) - 12}ê°œ ì„±ë¶„")

    lines.append("\nì¶œì²˜: ì˜ì•½í’ˆì•ˆì „ë‚˜ë¼")
    return "\n".join(lines)


def format_date_range_for_kakao(data: dict) -> str:
    """ê¸°ê°„ë³„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¹´ì¹´ì˜¤í†¡ ë©”ì‹œì§€ í˜•íƒœë¡œ í¬ë§·"""
    if data.get("ì´ê±´ìˆ˜", 0) == 0:
        return f"ğŸ“… {data['ê¸°ê°„']} DMF í˜„í™©\n\n{data.get('ë©”ì‹œì§€', 'ë“±ë¡ ì—†ìŒ')}"

    lines = [
        f"ğŸ“… {data['ê¸°ê°„']} DMF í˜„í™©",
        f"{'â”€'*24}",
        f"ì´ {data['ì´ê±´ìˆ˜']}ê±´ (ìµœì´ˆ {data['ìµœì´ˆë“±ë¡']} / í—ˆì—¬ {data['í—ˆì—¬']})",
        f"ì—°ê³„ì‹¬ì‚¬ {data['ì—°ê³„ì‹¬ì‚¬']}ê±´",
    ]

    country_dist = data.get("êµ­ê°€ë³„_ë¶„í¬", [])
    if country_dist:
        lines.append(f"\nğŸŒ êµ­ê°€ë³„:")
        for c in country_dist[:5]:
            lines.append(f"  {c['êµ­ê°€']}: {c['ê±´ìˆ˜']}ê±´")

    ingredients = data.get("ì„±ë¶„ë³„_í˜„í™©", [])
    if ingredients:
        lines.append(f"\nğŸ’Š ë“±ë¡ ì„±ë¶„:")
        for item in ingredients[:10]:
            lines.append(f"  {item['ì„±ë¶„ëª…'][:18]} ({item['êµ­ê°€']}) {item['ì‹ ì²­ì¸'][:8]}")

    if len(ingredients) > 10:
        lines.append(f"  ... ì™¸ {len(ingredients) - 10}ê°œ")

    lines.append("\nì¶œì²˜: ì˜ì•½í’ˆì•ˆì „ë‚˜ë¼")
    return "\n".join(lines)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Gemini Function Calling ë„êµ¬ ì •ì˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

GEMINI_TOOLS = [{
    "function_declarations": [
        {
            "name": "analyze_weekly_dmf",
            "description": "ì£¼ê°„ DMF ë“±ë¡ í˜„í™©ì„ ì¡°íšŒí•©ë‹ˆë‹¤. 'ì£¼ê°„', 'ì´ë²ˆì£¼', 'ê¸ˆì£¼' ë“±ì˜ ìš”ì²­ì— ì‚¬ìš©í•©ë‹ˆë‹¤.",
            "parameters": {
                "type": "object",
                "properties": {
                    "weeks_ago": {"type": "integer", "description": "ëª‡ ì£¼ ì „ (1=ì§€ë‚œì£¼, 0=ì´ë²ˆì£¼)"}
                }
            }
        },
        {
            "name": "analyze_monthly_dmf",
            "description": "ì›”ê°„ DMF ë“±ë¡ í˜„í™©ì„ ì¡°íšŒí•©ë‹ˆë‹¤. 'ì›”ê°„', 'ì´ë²ˆë‹¬', 'ì „ì›”' ë“±ì˜ ìš”ì²­ì— ì‚¬ìš©í•©ë‹ˆë‹¤.",
            "parameters": {
                "type": "object",
                "properties": {
                    "months_ago": {"type": "integer", "description": "ëª‡ ê°œì›” ì „ (1=ì „ì›”)"}
                }
            }
        },
        {
            "name": "search_ingredient",
            "description": "ì„±ë¶„ëª…ìœ¼ë¡œ DMF ë“±ë¡ í˜„í™©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤. ì œì¡°ì›, ì—°ê³„ì‹¬ì‚¬ í˜„í™©ì„ í¬í•¨í•©ë‹ˆë‹¤.",
            "parameters": {
                "type": "object",
                "properties": {
                    "ingredient": {"type": "string", "description": "ê²€ìƒ‰í•  ì„±ë¶„ëª… (ë¶€ë¶„ ë§¤ì¹­)"},
                    "linked_filter": {"type": "string", "description": "ì—°ê³„ì‹¬ì‚¬ í•„í„°: linked(ì—°ê³„ë§Œ), unlinked(ë¯¸ì—°ê³„ë§Œ), null(ì „ì²´)", "enum": ["linked", "unlinked"]}
                },
                "required": ["ingredient"]
            }
        },
        {
            "name": "search_country",
            "description": "íŠ¹ì • êµ­ê°€ì˜ DMF ë“±ë¡ í˜„í™©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
            "parameters": {
                "type": "object",
                "properties": {
                    "country": {"type": "string", "description": "êµ­ê°€ëª… (í•œêµ­ì–´: ì¸ë„, ì¤‘êµ­, ë¯¸êµ­ ë“±)"}
                },
                "required": ["country"]
            }
        },
        {
            "name": "search_applicant",
            "description": "ì‹ ì²­ì¸(ìˆ˜ì…ì‚¬)ë³„ DMF ë“±ë¡ í˜„í™©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
            "parameters": {
                "type": "object",
                "properties": {
                    "applicant": {"type": "string", "description": "ì‹ ì²­ì¸ëª… (ë¶€ë¶„ ë§¤ì¹­)"},
                    "month": {"type": "integer", "description": "íŠ¹ì • ì›” í•„í„° (1-12, ì„ íƒ)"}
                },
                "required": ["applicant"]
            }
        },
        {
            "name": "search_manufacturer",
            "description": "ì œì¡°ì†Œëª…ìœ¼ë¡œ DMF ë“±ë¡ í˜„í™©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
            "parameters": {
                "type": "object",
                "properties": {
                    "keyword": {"type": "string", "description": "ì œì¡°ì†Œëª… (ë¶€ë¶„ ë§¤ì¹­)"}
                },
                "required": ["keyword"]
            }
        },
        {
            "name": "search_date_range",
            "description": "íŠ¹ì • ê¸°ê°„ ë‚´ DMF ë“±ë¡ í˜„í™©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤. 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ìµœê·¼ Nì¼', '2ì›” 9ì¼ë¶€í„° ì˜¤ëŠ˜ê¹Œì§€' ë“±.",
            "parameters": {
                "type": "object",
                "properties": {
                    "start_date": {"type": "string", "description": "ì‹œì‘ì¼ (YYYY-MM-DD)"},
                    "end_date": {"type": "string", "description": "ì¢…ë£Œì¼ (YYYY-MM-DD)"}
                },
                "required": ["start_date", "end_date"]
            }
        },
        {
            "name": "compare_countries",
            "description": "ë‘ êµ­ê°€ì˜ DMF ë“±ë¡ í˜„í™©ì„ ë¹„êµí•©ë‹ˆë‹¤. 'ì¸ë„ vs ì¤‘êµ­', 'ì¸ë„ì™€ ì¤‘êµ­ ë¹„êµ' ë“±.",
            "parameters": {
                "type": "object",
                "properties": {
                    "country_a": {"type": "string", "description": "ì²« ë²ˆì§¸ êµ­ê°€ëª…"},
                    "country_b": {"type": "string", "description": "ë‘ ë²ˆì§¸ êµ­ê°€ëª…"}
                },
                "required": ["country_a", "country_b"]
            }
        },
        {
            "name": "get_top_rankings",
            "description": "ì¹´í…Œê³ ë¦¬ë³„ ìƒìœ„ ë­í‚¹ì„ ì¡°íšŒí•©ë‹ˆë‹¤. 'ê°€ì¥ ë§ì´ ë“±ë¡ëœ ì„±ë¶„ TOP 10', 'ì£¼ìš” êµ­ê°€ ìˆœìœ„' ë“±.",
            "parameters": {
                "type": "object",
                "properties": {
                    "category": {"type": "string", "description": "ì¹´í…Œê³ ë¦¬", "enum": ["ingredient", "country", "applicant", "manufacturer"]},
                    "top_n": {"type": "integer", "description": "ìƒìœ„ Nê°œ (ê¸°ë³¸ 10)"},
                    "period_months": {"type": "integer", "description": "ìµœê·¼ Nê°œì›” í•„í„° (ì„ íƒ, ì—†ìœ¼ë©´ ì „ì²´ ê¸°ê°„)"}
                },
                "required": ["category"]
            }
        },
        {
            "name": "generate_chat_summary",
            "description": "ì¹´ì¹´ì˜¤í†¡ ê³µìœ ìš© ê°„ê²°í•œ DMF ì£¼ê°„ ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
        }
    ]
}]


def _execute_gemini_function(name: str, args: dict) -> str:
    """Geminiê°€ ìš”ì²­í•œ í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ê³  JSON ë¬¸ìì—´ë¡œ ë°˜í™˜"""
    try:
        if name == 'analyze_weekly_dmf':
            result = analyze_weekly_dmf(args.get('weeks_ago', 1))
        elif name == 'analyze_monthly_dmf':
            result = analyze_monthly_dmf(args.get('months_ago', 1))
        elif name == 'search_ingredient':
            result = search_ingredient(args['ingredient'], args.get('linked_filter'))
        elif name == 'search_country':
            result = search_country(args['country'])
        elif name == 'search_applicant':
            result = search_applicant(args['applicant'], args.get('month'))
        elif name == 'search_manufacturer':
            result = search_manufacturer(args['keyword'])
        elif name == 'search_date_range':
            start = datetime.strptime(args['start_date'], '%Y-%m-%d')
            end = datetime.strptime(args['end_date'], '%Y-%m-%d')
            result = search_date_range(start, end)
        elif name == 'compare_countries':
            result = compare_countries(args['country_a'], args['country_b'])
        elif name == 'get_top_rankings':
            result = get_top_rankings(args['category'], args.get('top_n', 10), args.get('period_months'))
        elif name == 'generate_chat_summary':
            return generate_chat_summary()
        else:
            return json.dumps({"error": f"ì•Œ ìˆ˜ ì—†ëŠ” í•¨ìˆ˜: {name}"}, ensure_ascii=False)
        return json.dumps(result, ensure_ascii=False, default=str)
    except Exception as e:
        return json.dumps({"error": str(e)}, ensure_ascii=False)


def handle_with_gemini(utterance: str) -> Optional[str]:
    """
    Gemini 2.0 Flash Function Calling + ìì—°ì–´ ì‘ë‹µ ìƒì„±

    Flow:
    1. ì‚¬ìš©ì ë©”ì‹œì§€ + ë°ì´í„° ìš”ì•½ + ë„êµ¬ ì„ ì–¸ì„ Geminiì— ì „ì†¡
    2. Geminiê°€ function_call ë°˜í™˜ â†’ ë¡œì»¬ ì‹¤í–‰ â†’ ê²°ê³¼ë¥¼ Geminiì— ë‹¤ì‹œ ì „ì†¡
    3. Geminiê°€ ìì—°ì–´ ì‘ë‹µ ìƒì„± â†’ ë°˜í™˜
    4. ì‹¤íŒ¨ ì‹œ None â†’ regex fallback

    Returns: ì¹´ì¹´ì˜¤í†¡ìš© í…ìŠ¤íŠ¸ ì‘ë‹µ ë˜ëŠ” None
    """
    api_key = os.environ.get("GEMINI_API_KEY", "")
    if not api_key:
        return None

    today = datetime.today()
    digest = _cache.get("digest", "ë°ì´í„° ë¡œë”© ì¤‘")

    system_prompt = f"""ë‹¹ì‹ ì€ DMF Intelligence, í•œêµ­ ì˜ì•½í’ˆì•ˆì „ë‚˜ë¼ì˜ DMF(Drug Master File) ë“±ë¡ ë°ì´í„° ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ê·œì¹™:
1. í•­ìƒ í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
2. ì‘ë‹µì€ 3500ì ì´ë‚´ë¡œ ìœ ì§€í•˜ì„¸ìš” (ì¹´ì¹´ì˜¤í†¡ ì œí•œ).
3. ë°ì´í„° ì¡°íšŒê°€ í•„ìš”í•˜ë©´ ì œê³µëœ ë„êµ¬(í•¨ìˆ˜)ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.
4. ì¼ë°˜ì ì¸ DMF ì§€ì‹ ì§ˆë¬¸ì€ ë„êµ¬ ì—†ì´ ì§ì ‘ ë‹µë³€í•˜ì„¸ìš”.
5. ì•„ë˜ ë°ì´í„° ìš”ì•½ì„ ì°¸ê³ í•˜ì—¬ ë¶„ì„/ë¹„êµ/ë­í‚¹ ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
6. ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ë˜ ê³¼í•˜ì§€ ì•Šê²Œ í•˜ì„¸ìš”.
7. ì¶œì²˜: ì˜ì•½í’ˆì•ˆì „ë‚˜ë¼ë¥¼ ì‘ë‹µ ëì— í¬í•¨í•˜ì„¸ìš”.
8. ëª¨ë¥´ëŠ” ê²ƒì€ ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  í•˜ì„¸ìš”.

ì˜¤ëŠ˜ ë‚ ì§œ: {today.strftime('%Y-%m-%d')}

ë°ì´í„° ìš”ì•½:
{digest}"""

    api_url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}"

    try:
        # Step 1: ì‚¬ìš©ì ë©”ì‹œì§€ + ë„êµ¬ ì „ì†¡
        resp = requests.post(api_url, json={
            "system_instruction": {"parts": [{"text": system_prompt}]},
            "contents": [{"role": "user", "parts": [{"text": utterance}]}],
            "tools": GEMINI_TOOLS,
            "generationConfig": {"temperature": 0.3, "maxOutputTokens": 1000}
        }, timeout=3.5)

        if resp.status_code != 200:
            logger.warning(f"Gemini API ì‹¤íŒ¨: {resp.status_code}")
            return None

        result = resp.json()
        candidate = result['candidates'][0]['content']
        parts = candidate.get('parts', [])

        if not parts:
            return None

        # Case A: Geminiê°€ ì§ì ‘ í…ìŠ¤íŠ¸ë¡œ ì‘ë‹µ (ë„êµ¬ í˜¸ì¶œ ë¶ˆí•„ìš”)
        if 'text' in parts[0]:
            text = parts[0]['text'].strip()
            if len(text) > 3500:
                text = text[:3450] + "\n\n... (ì‘ë‹µì´ ì˜ë ¸ìŠµë‹ˆë‹¤)"
            logger.info(f"ğŸ¤– Gemini ì§ì ‘ ì‘ë‹µ ({len(text)}ì)")
            return text

        # Case B: Geminiê°€ í•¨ìˆ˜ í˜¸ì¶œ ìš”ì²­
        if 'functionCall' in parts[0]:
            fc = parts[0]['functionCall']
            fn_name = fc['name']
            fn_args = fc.get('args', {})
            logger.info(f"ğŸ”§ Gemini í•¨ìˆ˜ í˜¸ì¶œ: {fn_name}({fn_args})")

            # í•¨ìˆ˜ ì‹¤í–‰
            fn_result = _execute_gemini_function(fn_name, fn_args)

            # Step 2: í•¨ìˆ˜ ê²°ê³¼ë¥¼ Geminiì— ë³´ë‚´ì„œ ìì—°ì–´ ì‘ë‹µ ìƒì„±
            resp2 = requests.post(api_url, json={
                "system_instruction": {"parts": [{"text": system_prompt}]},
                "contents": [
                    {"role": "user", "parts": [{"text": utterance}]},
                    {"role": "model", "parts": [{"functionCall": {"name": fn_name, "args": fn_args}}]},
                    {"role": "user", "parts": [{"functionResponse": {"name": fn_name, "response": {"result": fn_result}}}]}
                ],
                "tools": GEMINI_TOOLS,
                "generationConfig": {"temperature": 0.3, "maxOutputTokens": 1000}
            }, timeout=3.5)

            if resp2.status_code != 200:
                logger.warning(f"Gemini 2ì°¨ ì‘ë‹µ ì‹¤íŒ¨: {resp2.status_code}")
                return None

            result2 = resp2.json()
            parts2 = result2['candidates'][0]['content'].get('parts', [])
            if parts2 and 'text' in parts2[0]:
                text = parts2[0]['text'].strip()
                if len(text) > 3500:
                    text = text[:3450] + "\n\n... (ì‘ë‹µì´ ì˜ë ¸ìŠµë‹ˆë‹¤)"
                logger.info(f"ğŸ¤– Gemini í•¨ìˆ˜ ê¸°ë°˜ ì‘ë‹µ ({len(text)}ì)")
                return text

        return None

    except requests.Timeout:
        logger.warning("Gemini API íƒ€ì„ì•„ì›ƒ")
        return None
    except Exception as e:
        logger.warning(f"Gemini ëŒ€í™” ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return None


def parse_user_intent(utterance: str) -> tuple:
    """
    ì‚¬ìš©ì ë°œí™”ë¥¼ ë¶„ì„í•˜ì—¬ ì˜ë„ì™€ íŒŒë¼ë¯¸í„° ì¶”ì¶œ

    Returns:
        (intent, params) íŠœí”Œ
        intent: 'weekly' | 'monthly' | 'date_range' | 'ingredient' | 'country' | 'applicant' | 'summary' | 'help'
    """
    text = utterance.strip().lower()
    today = datetime.today()

    # â”€â”€â”€ 1. ì¸ì‚¬ / ë„ì›€ë§ â”€â”€â”€
    if text in ['ì•ˆë…•', 'í•˜ì´', 'hi', 'hello', 'ì‹œì‘', ''] or len(text) <= 1:
        return ('help', {})
    if any(kw in text for kw in ['ë„ì›€', 'ì‚¬ìš©ë²•', 'ì•ˆë‚´', 'ë©”ë‰´', 'ë­˜ í•  ìˆ˜', 'ê¸°ëŠ¥', 'ëª…ë ¹', 'ë­ í• ', 'ë­˜ ë¬¼', 'ì–´ë–»ê²Œ']):
        return ('help', {})

    # â”€â”€â”€ 2. ìš”ì•½ â”€â”€â”€
    if any(kw in text for kw in ['ìš”ì•½', 'ê³µìœ ', 'ì •ë¦¬', 'ì¹´í†¡', 'ì±—']):
        return ('summary', {})

    # â”€â”€â”€ 3. ë‚ ì§œ/ê¸°ê°„ ê´€ë ¨ ê²€ìƒ‰ â”€â”€â”€
    # "Nì›” Nì¼ë¶€í„° (ì˜¤ëŠ˜/Nì›” Nì¼)ê¹Œì§€" (êµ¬ì²´ì  ë²”ìœ„ ë¨¼ì €!)
    range_match = re.search(r'(\d{1,2})ì›”\s*(\d{1,2})ì¼\s*ë¶€í„°\s*(?:ì˜¤ëŠ˜|(\d{1,2})ì›”\s*(\d{1,2})ì¼)\s*ê¹Œì§€', text)
    if range_match:
        sm, sd = int(range_match.group(1)), int(range_match.group(2))
        start = datetime(today.year, sm, sd)
        if range_match.group(3):
            em, ed = int(range_match.group(3)), int(range_match.group(4))
            end = datetime(today.year, em, ed)
        else:
            end = today
        return ('date_range', {'start': start, 'end': end})

    # "Nì¼ë¶€í„° ì˜¤ëŠ˜ê¹Œì§€" (ì›” ìƒëµ)
    range_match2 = re.search(r'(\d{1,2})ì¼\s*ë¶€í„°\s*ì˜¤ëŠ˜\s*ê¹Œì§€', text)
    if range_match2:
        day = int(range_match2.group(1))
        month_ctx = re.search(r'(\d{1,2})ì›”', text)
        m = int(month_ctx.group(1)) if month_ctx else today.month
        start = datetime(today.year, m, day)
        return ('date_range', {'start': start, 'end': today})

    # "ìµœê·¼ Nì¼"
    recent_match = re.search(r'ìµœê·¼\s*(\d+)\s*ì¼', text)
    if recent_match:
        days = int(recent_match.group(1))
        return ('date_range', {'start': today - timedelta(days=days), 'end': today})

    # "ì˜¤ëŠ˜ ë“±ë¡", "ì–´ì œ í˜„í™©" (ì¼ë°˜ì  ì˜¤ëŠ˜/ì–´ì œ)
    if re.search(r'ì˜¤ëŠ˜.*(ë“±ë¡|dmf|í˜„í™©|ì‹ ê·œ)', text) or re.search(r'(ë“±ë¡|dmf|í˜„í™©|ì‹ ê·œ).*ì˜¤ëŠ˜', text):
        return ('date_range', {'start': today, 'end': today})

    if re.search(r'ì–´ì œ.*(ë“±ë¡|dmf|í˜„í™©|ì‹ ê·œ)', text) or re.search(r'(ë“±ë¡|dmf|í˜„í™©|ì‹ ê·œ).*ì–´ì œ', text):
        yesterday = today - timedelta(days=1)
        return ('date_range', {'start': yesterday, 'end': yesterday})

    # "ì´ë²ˆì£¼", "ì£¼ê°„", "ê¸ˆì£¼"
    if any(kw in text for kw in ['ì£¼ê°„', 'ì´ë²ˆì£¼', 'ì´ë²ˆ ì£¼', 'ê¸ˆì£¼', 'ì§€ë‚œì£¼', 'ì§€ë‚œ ì£¼', 'ì£¼ë³„']):
        return ('weekly', {})

    # "ì›”ê°„", "ì´ë²ˆë‹¬"
    if any(kw in text for kw in ['ì›”ê°„', 'ì´ë²ˆë‹¬', 'ì´ë²ˆ ë‹¬', 'ì „ì›”', 'ì§€ë‚œë‹¬', 'ì§€ë‚œ ë‹¬', 'ì›”ë³„']):
        return ('monthly', {})

    # â”€â”€â”€ 4. ì‹ ì²­ì¸ ê²€ìƒ‰ â”€â”€â”€
    month_match = re.search(r'(\d{1,2})ì›”', text)
    month = int(month_match.group(1)) if month_match else None

    applicant_match = re.search(r'(?:ì‹ ì²­ì¸|ìˆ˜ì…ì‚¬|ìˆ˜ì…ì—…ì²´|ê±°ë˜ì²˜)\s*[:]?\s*(.+?)(?:\s*(?:í˜„í™©|ê²€ìƒ‰|ì¡°íšŒ|dmf|ë“±ë¡|ëª‡ê°œ|ê°¯ìˆ˜).*$|\s*\??\s*$)', text)
    if applicant_match:
        name = applicant_match.group(1).strip()
        if name:
            return ('applicant', {'applicant': name, 'month': month})

    # â”€â”€â”€ 5. êµ­ê°€ ê²€ìƒ‰ â”€â”€â”€
    country_keywords = ['ì¸ë„', 'ì¤‘êµ­', 'ì¼ë³¸', 'ë¯¸êµ­', 'ë…ì¼', 'ì´íƒˆë¦¬ì•„', 'ìŠ¤í˜ì¸',
                        'í”„ë‘ìŠ¤', 'ì˜êµ­', 'ìºë‚˜ë‹¤', 'ë¸Œë¼ì§ˆ', 'ëŒ€ë§Œ', 'í•œêµ­', 'ì´ìŠ¤ë¼ì—˜']
    for kw in country_keywords:
        if kw in text:
            return ('country', {'country': kw})

    # â”€â”€â”€ 6. ì„±ë¶„/í†µí•© ê²€ìƒ‰ â”€â”€â”€
    linked_filter = None
    if any(kw in text for kw in ['ì—°ê³„ ì•ˆ', 'ë¯¸ì—°ê³„', 'ì—°ê³„ì•ˆ', 'ë¹„ì—°ê³„', 'ì—°ê³„ ì—†']):
        linked_filter = 'unlinked'
    elif any(kw in text for kw in ['ì—°ê³„ì‹¬ì‚¬', 'ì—°ê³„', 'linked']):
        linked_filter = 'linked'

    # ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ (ëª¨ë“  ë¶ˆí•„ìš” ë‹¨ì–´ ì œê±°)
    clean_text = re.sub(
        r'(?:ì—°ê³„ì‹¬ì‚¬|ë¯¸ì—°ê³„|ë¹„ì—°ê³„|ì—°ê³„|ì œì¡°ì›|ì œì¡°ì‚¬|í˜„í™©|ê²€ìƒ‰|ì¡°íšŒ|dmf|ë“±ë¡|í—ˆì—¬|ì‹ ê·œ|'
        r'ëœ|ì•ˆëœ|ìˆëŠ”|ì—†ëŠ”|ëª‡ê°œ|ê°¯ìˆ˜|ìˆ˜|ì•Œë ¤ì¤˜|ë³´ì—¬ì¤˜|ë­ì•¼|ì¢€|í•´ì¤˜|'
        r'ë¶€í„°|ê¹Œì§€|ì˜¤ëŠ˜|ì–´ì œ|ìµœê·¼|ê¸°ê°„|ì¤‘ì—ì„œ|ì¤‘|ì—ì„œ|ì˜|ì—)',
        ' ', text
    ).strip()
    clean_text = re.sub(r'\d{1,2}ì›”\s*(?:\d{1,2}ì¼)?\s*(?:ì—|ì˜|ì€|ëŠ”)?\s*', '', clean_text).strip()
    clean_text = re.sub(r'\d{1,2}ì¼', '', clean_text).strip()
    clean_text = re.sub(r'\s+', ' ', clean_text).strip()
    clean_text = re.sub(r'[?ï¼Ÿ!]$', '', clean_text).strip()
    clean_text = re.sub(r'(?:ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼)$', '', clean_text).strip()

    if clean_text and clean_text not in ['ì•ˆë…•', 'í•˜ì´', 'hi', 'hello', 'ì‹œì‘'] and len(clean_text) > 1:
        return ('ingredient', {'ingredient': clean_text, 'linked_filter': linked_filter, 'month': month})

    # â”€â”€â”€ 7. ìœ„ ëª¨ë“  ê²ƒì— í•´ë‹¹ ì•ˆ ë˜ë©´ â”€â”€â”€
    # ë‚ ì§œ ê´€ë ¨ ë‹¨ì–´ë§Œ ìˆì—ˆìœ¼ë©´ â†’ ì£¼ê°„ í˜„í™©ìœ¼ë¡œ
    if any(kw in text for kw in ['ë“±ë¡', 'í˜„í™©', 'dmf', 'ì‹ ê·œ']):
        return ('weekly', {})

    return ('help', {})


# â”€â”€â”€ ì¹´ì¹´ì˜¤ ì›¹í›… ì—”ë“œí¬ì¸íŠ¸ë“¤ â”€â”€â”€

@app.get("/")
async def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {
        "status": "running",
        "service": "DMF Intelligence Server",
        "cache": "loaded" if _cache["df"] is not None else "empty",
        "last_updated": str(_cache["last_updated"]) if _cache["last_updated"] else None,
        "endpoints": {
            "kakao_webhook": "/kakao/skill",
            "mcp_sse": "/sse" if MCP_AVAILABLE else "not available"
        }
    }


@app.get("/refresh")
async def refresh_cache():
    """ìºì‹œ ê°•ì œ ê°±ì‹  (Cron Jobìš©) â€” ë§¤ì¼ ì•„ì¹¨ 7ì‹œ í˜¸ì¶œ"""
    try:
        _cache["df"] = None
        _cache["last_updated"] = None
        _get_cached_data()
        return {
            "status": "refreshed",
            "records": len(_cache["df"]),
            "updated_at": str(_cache["last_updated"])
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}


@app.post("/kakao/skill")
async def kakao_skill_handler(request: Request):
    """
    ì¹´ì¹´ì˜¤ i ì˜¤í”ˆë¹Œë” í†µí•© Skill ì—”ë“œí¬ì¸íŠ¸
    
    ì‚¬ìš©ì ë°œí™”ë¥¼ ìë™ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ DMF ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì˜¤í”ˆë¹Œë”ì˜ 'í´ë°± ë¸”ë¡'ì— ì—°ê²°í•˜ë©´, ëª¨ë“  ì…ë ¥ì„ ì—¬ê¸°ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    try:
        body = await request.json()
        utterance = body.get("userRequest", {}).get("utterance", "")
        params = body.get("action", {}).get("params", {})

        logger.info(f"ğŸ“¨ ì¹´ì¹´ì˜¤ ìš”ì²­: '{utterance}' | params: {params}")

        # ìºì‹œê°€ ì•„ì§ ì¤€ë¹„ ì•ˆ ëìœ¼ë©´ ì¦‰ì‹œ ì•ˆë‚´
        if _cache["df"] is None and _cache["loading"]:
            return JSONResponse(kakao_simple_text(
                "ğŸ”„ ì„œë²„ê°€ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.\n10ì´ˆ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”!"
            ))

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 3-Tier ë¼ìš°íŒ…
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        # â”€â”€â”€ Tier 1: Regex ë¹ ë¥¸ ê²½ë¡œ (<100ms) â”€â”€â”€
        intent, extracted = parse_user_intent(utterance)
        logger.info(f"ğŸ“ Regex: intent={intent}, params={extracted}")

        if intent == 'weekly':
            data = analyze_weekly_dmf()
            text = format_weekly_for_kakao(data)
            return JSONResponse(kakao_quick_replies(text, [
                {"messageText": "ì›”ê°„", "action": "message", "label": "ğŸ“Š ì›”ê°„ ë¦¬í¬íŠ¸"},
                {"messageText": "ìš”ì•½", "action": "message", "label": "ğŸ“‹ ì±„íŒ… ê³µìœ ìš©"},
                {"messageText": "ë„ì›€", "action": "message", "label": "â“ ì‚¬ìš©ë²•"}
            ]))

        elif intent == 'monthly':
            data = analyze_monthly_dmf()
            text = format_monthly_for_kakao(data)
            return JSONResponse(kakao_quick_replies(text, [
                {"messageText": "ì£¼ê°„", "action": "message", "label": "ğŸ“‹ ì£¼ê°„ í˜„í™©"},
                {"messageText": "ì¸ë„", "action": "message", "label": "ğŸ‡®ğŸ‡³ ì¸ë„ DMF"},
                {"messageText": "ë„ì›€", "action": "message", "label": "â“ ì‚¬ìš©ë²•"}
            ]))

        elif intent == 'summary':
            text = generate_chat_summary()
            return JSONResponse(kakao_simple_text(text))

        elif intent == 'date_range':
            start = extracted.get('start', datetime.today())
            end = extracted.get('end', datetime.today())
            data = search_date_range(start, end)
            text = format_date_range_for_kakao(data)
            return JSONResponse(kakao_quick_replies(text, [
                {"messageText": "ì£¼ê°„", "action": "message", "label": "ğŸ“‹ ì£¼ê°„ í˜„í™©"},
                {"messageText": "ì›”ê°„", "action": "message", "label": "ğŸ“Š ì›”ê°„ ë¦¬í¬íŠ¸"},
                {"messageText": "ë„ì›€", "action": "message", "label": "â“ ë©”ë‰´"}
            ]))

        elif intent == 'country':
            country = extracted.get('country', params.get('country', ''))
            if not country:
                return JSONResponse(kakao_simple_text("ì–´ëŠ êµ­ê°€ì˜ DMFë¥¼ ê²€ìƒ‰í• ê¹Œìš”?\n\nì˜ˆ: ì¸ë„, ì¤‘êµ­, ì¼ë³¸, ë¯¸êµ­"))
            data = search_country(country)
            text = format_country_for_kakao(data)
            return JSONResponse(kakao_quick_replies(text, [
                {"messageText": "ì£¼ê°„", "action": "message", "label": "ğŸ“‹ ì£¼ê°„ í˜„í™©"},
                {"messageText": "ì›”ê°„", "action": "message", "label": "ğŸ“Š ì›”ê°„ ë¦¬í¬íŠ¸"},
                {"messageText": "ë„ì›€", "action": "message", "label": "â“ ë©”ë‰´"}
            ]))

        elif intent == 'applicant':
            applicant = extracted.get('applicant', params.get('applicant', ''))
            month = extracted.get('month')
            if not applicant:
                return JSONResponse(kakao_simple_text("ê²€ìƒ‰í•  ì‹ ì²­ì¸ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\n\nì˜ˆ: ì‹ ì²­ì¸ íœ´ì‹œë“œ\nì˜ˆ: 1ì›”ì— ì‹ ì²­ì¸ êµ­ì „ì•½í’ˆ í˜„í™©"))
            data = search_applicant(applicant, month)
            text = format_applicant_for_kakao(data)
            return JSONResponse(kakao_quick_replies(text, [
                {"messageText": "ì£¼ê°„", "action": "message", "label": "ğŸ“‹ ì£¼ê°„ í˜„í™©"},
                {"messageText": "ì›”ê°„", "action": "message", "label": "ğŸ“Š ì›”ê°„ ë¦¬í¬íŠ¸"},
                {"messageText": "ë„ì›€", "action": "message", "label": "â“ ë©”ë‰´"}
            ]))

        elif intent == 'ingredient':
            keyword = extracted.get('ingredient', params.get('ingredient', ''))
            linked_filter = extracted.get('linked_filter')
            if not keyword:
                return JSONResponse(kakao_simple_text("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\n\nì˜ˆ: í´ë˜ë¦¬, Synthimed, íœ´ì‹œë“œ"))

            # ì—°ê³„ í•„í„°ê°€ ìˆìœ¼ë©´ ì„±ë¶„ëª… ê²€ìƒ‰ ê³ ì •
            if linked_filter:
                data = search_ingredient(keyword, linked_filter)
                text = format_ingredient_for_kakao(data)
                replies = []
                if linked_filter != 'linked':
                    replies.append({"messageText": f"{keyword} ì—°ê³„ì‹¬ì‚¬", "action": "message", "label": "âœ… ì—°ê³„ì‹¬ì‚¬ë§Œ"})
                if linked_filter != 'unlinked':
                    replies.append({"messageText": f"{keyword} ë¯¸ì—°ê³„", "action": "message", "label": "â¬œ ë¯¸ì—°ê³„ë§Œ"})
                replies.append({"messageText": keyword, "action": "message", "label": "ğŸ“‹ ì „ì²´ ë³´ê¸°"})
                replies.append({"messageText": "ë„ì›€", "action": "message", "label": "â“ ë©”ë‰´"})
                return JSONResponse(kakao_quick_replies(text, replies[:4]))

            # í†µí•© ê²€ìƒ‰: ì„±ë¶„ëª… â†’ ì‹ ì²­ì¸ â†’ ì œì¡°ì†Œëª…
            month = extracted.get('month')
            search_type, uni_data = search_universal(keyword, month)

            if search_type == 'ingredient':
                data = search_ingredient(keyword)
                text = format_ingredient_for_kakao(data)
                return JSONResponse(kakao_quick_replies(text, [
                    {"messageText": f"{keyword} ì—°ê³„ì‹¬ì‚¬", "action": "message", "label": "âœ… ì—°ê³„ì‹¬ì‚¬ë§Œ"},
                    {"messageText": f"{keyword} ë¯¸ì—°ê³„", "action": "message", "label": "â¬œ ë¯¸ì—°ê³„ë§Œ"},
                    {"messageText": "ë„ì›€", "action": "message", "label": "â“ ë©”ë‰´"}
                ]))

            elif search_type == 'applicant':
                text = format_applicant_for_kakao(uni_data)
                return JSONResponse(kakao_quick_replies(text, [
                    {"messageText": "ì£¼ê°„", "action": "message", "label": "ğŸ“‹ ì£¼ê°„ í˜„í™©"},
                    {"messageText": "ì›”ê°„", "action": "message", "label": "ğŸ“Š ì›”ê°„ ë¦¬í¬íŠ¸"},
                    {"messageText": "ë„ì›€", "action": "message", "label": "â“ ë©”ë‰´"}
                ]))

            elif search_type == 'manufacturer':
                text = format_manufacturer_for_kakao(uni_data)
                return JSONResponse(kakao_quick_replies(text, [
                    {"messageText": "ì£¼ê°„", "action": "message", "label": "ğŸ“‹ ì£¼ê°„ í˜„í™©"},
                    {"messageText": "ì›”ê°„", "action": "message", "label": "ğŸ“Š ì›”ê°„ ë¦¬í¬íŠ¸"},
                    {"messageText": "ë„ì›€", "action": "message", "label": "â“ ë©”ë‰´"}
                ]))

            else:
                # í†µí•© ê²€ìƒ‰ ì‹¤íŒ¨ â†’ Tier 2/3ìœ¼ë¡œ ì „ë‹¬
                pass

        # â”€â”€â”€ Tier 2/3: Gemini ëŒ€í™”í˜• (intent == 'help' ë˜ëŠ” ê²€ìƒ‰ ì‹¤íŒ¨) â”€â”€â”€
        # Gemini Function Calling + ìì—°ì–´ ì‘ë‹µ ìƒì„±
        gemini_response = handle_with_gemini(utterance)
        if gemini_response:
            logger.info(f"ğŸ¤– Gemini ëŒ€í™”í˜• ì‘ë‹µ ì‚¬ìš©")
            return JSONResponse(kakao_quick_replies(gemini_response, [
                {"messageText": "ì£¼ê°„", "action": "message", "label": "ğŸ“‹ ì£¼ê°„ í˜„í™©"},
                {"messageText": "ì›”ê°„", "action": "message", "label": "ğŸ“Š ì›”ê°„ ë¦¬í¬íŠ¸"},
                {"messageText": "ìµœê·¼ 3ì¼", "action": "message", "label": "ğŸ“… ìµœê·¼ 3ì¼"},
                {"messageText": "ë„ì›€", "action": "message", "label": "â“ ë©”ë‰´"}
            ]))

        # â”€â”€â”€ Fallback: Geminië„ ì‹¤íŒ¨í•˜ë©´ ë„ì›€ë§ â”€â”€â”€
        help_text = (
            "ğŸ’Š DMF Intelligence\n"
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            "ì˜ì•½í’ˆì•ˆì „ë‚˜ë¼ DMF ë°ì´í„°ë¥¼\n"
            "ì‹¤ì‹œê°„ìœ¼ë¡œ ì¡°íšŒÂ·ë¶„ì„í•©ë‹ˆë‹¤.\n\n"
            "ì•„ë˜ ë²„íŠ¼ì„ ëˆ„ë¥´ê±°ë‚˜ ì§ì ‘ ì…ë ¥í•˜ì„¸ìš”!\n\n"
            "ğŸ’¡ ì…ë ¥ ì˜ˆì‹œ:\n"
            "â€¢ ì„¸íŒŒí´ëŸ¬ â†’ ì œì¡°ì› í˜„í™©\n"
            "â€¢ ì„¸íŒŒí´ëŸ¬ ì—°ê³„ì‹¬ì‚¬ â†’ ì—°ê³„ ì œì¡°ì›ë§Œ\n"
            "â€¢ íœ´ì‹œë“œ â†’ ì‹ ì²­ì¸ ê²€ìƒ‰\n"
            "â€¢ ì¸ë„ â†’ êµ­ê°€ë³„ DMF í˜„í™©\n"
            "â€¢ ì¸ë„ vs ì¤‘êµ­ â†’ êµ­ê°€ ë¹„êµ\n"
            "â€¢ ì„±ë¶„ ë­í‚¹ â†’ TOP 10 ì„±ë¶„\n"
            "â€¢ 2ì›”9ì¼ë¶€í„° ì˜¤ëŠ˜ê¹Œì§€ â†’ ê¸°ê°„ ê²€ìƒ‰\n"
            "â€¢ DMFê°€ ë­ì•¼? â†’ ë¬´ì—‡ì´ë“  ì§ˆë¬¸\n"
            "â€¢ ìµœê·¼ 3ì¼ â†’ ìµœê·¼ ë“±ë¡ í˜„í™©"
        )
        return JSONResponse(kakao_quick_replies(help_text, [
            {"messageText": "ì£¼ê°„", "action": "message", "label": "ğŸ“‹ ì£¼ê°„ í˜„í™©"},
            {"messageText": "ì›”ê°„", "action": "message", "label": "ğŸ“Š ì›”ê°„ ë¦¬í¬íŠ¸"},
            {"messageText": "ìµœê·¼ 3ì¼", "action": "message", "label": "ğŸ“… ìµœê·¼ 3ì¼"},
            {"messageText": "ì¸ë„", "action": "message", "label": "ğŸ‡®ğŸ‡³ ì¸ë„ DMF"}
        ]))

    except Exception as e:
        logger.error(f"âŒ ì¹´ì¹´ì˜¤ ìŠ¤í‚¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return JSONResponse(kakao_simple_text(
            f"âš ï¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\nì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n\n(ì˜¤ë¥˜: {str(e)[:100]})"
        ))


# ê°œë³„ ìŠ¤í‚¬ ì—”ë“œí¬ì¸íŠ¸ (ì˜¤í”ˆë¹Œë”ì—ì„œ ë¸”ë¡ë³„ë¡œ ì—°ê²°í•  ë•Œ ì‚¬ìš©)
@app.post("/kakao/weekly")
async def kakao_weekly(request: Request):
    """ì£¼ê°„ DMF í˜„í™© ì „ìš© ìŠ¤í‚¬"""
    try:
        data = analyze_weekly_dmf()
        text = format_weekly_for_kakao(data)
        return JSONResponse(kakao_simple_text(text))
    except Exception as e:
        return JSONResponse(kakao_simple_text(f"âš ï¸ ì¡°íšŒ ì‹¤íŒ¨: {str(e)[:100]}"))


@app.post("/kakao/monthly")
async def kakao_monthly(request: Request):
    """ì›”ê°„ DMF ë¦¬í¬íŠ¸ ì „ìš© ìŠ¤í‚¬"""
    try:
        data = analyze_monthly_dmf()
        text = format_monthly_for_kakao(data)
        return JSONResponse(kakao_simple_text(text))
    except Exception as e:
        return JSONResponse(kakao_simple_text(f"âš ï¸ ì¡°íšŒ ì‹¤íŒ¨: {str(e)[:100]}"))


@app.post("/kakao/summary")
async def kakao_summary(request: Request):
    """ì±„íŒ… ê³µìœ ìš© ìš”ì•½ ì „ìš© ìŠ¤í‚¬"""
    try:
        text = generate_chat_summary()
        return JSONResponse(kakao_simple_text(text))
    except Exception as e:
        return JSONResponse(kakao_simple_text(f"âš ï¸ ìš”ì•½ ì‹¤íŒ¨: {str(e)[:100]}"))


@app.post("/kakao/ingredient")
async def kakao_ingredient(request: Request):
    """ì„±ë¶„ëª… ê²€ìƒ‰ ì „ìš© ìŠ¤í‚¬ (íŒŒë¼ë¯¸í„°: ingredient)"""
    try:
        body = await request.json()
        utterance = body.get("userRequest", {}).get("utterance", "")
        ingredient = body.get("action", {}).get("params", {}).get("ingredient", utterance)

        if not ingredient:
            return JSONResponse(kakao_simple_text("ê²€ìƒ‰í•  ì„±ë¶„ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."))

        data = search_ingredient(ingredient)
        text = format_ingredient_for_kakao(data)
        return JSONResponse(kakao_simple_text(text))
    except Exception as e:
        return JSONResponse(kakao_simple_text(f"âš ï¸ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)[:100]}"))


@app.post("/kakao/country")
async def kakao_country(request: Request):
    """êµ­ê°€ ê²€ìƒ‰ ì „ìš© ìŠ¤í‚¬ (íŒŒë¼ë¯¸í„°: country)"""
    try:
        body = await request.json()
        utterance = body.get("userRequest", {}).get("utterance", "")
        country = body.get("action", {}).get("params", {}).get("country", utterance)

        if not country:
            return JSONResponse(kakao_simple_text("ê²€ìƒ‰í•  êµ­ê°€ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."))

        data = search_country(country)
        text = format_country_for_kakao(data)
        return JSONResponse(kakao_simple_text(text))
    except Exception as e:
        return JSONResponse(kakao_simple_text(f"âš ï¸ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)[:100]}"))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„œë²„ ì‹¤í–‰ (MCP + ì¹´ì¹´ì˜¤ ë™ì‹œ ì§€ì›)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    mode = os.environ.get("SERVER_MODE", "kakao")  # "kakao" | "mcp" | "both"

    if mode == "mcp" and MCP_AVAILABLE:
        # MCP ì „ìš© ëª¨ë“œ (Claude Desktop / PlayMCP)
        print(f"ğŸš€ DMF MCP Server (SSE) ì‹œì‘ â€” Port {port}")
        mcp.run(transport="sse", port=port)

    elif mode == "both" and MCP_AVAILABLE:
        # ë‘ ì„œë²„ ë™ì‹œ ì‹¤í–‰ (ë³„ë„ í¬íŠ¸)
        import threading
        mcp_port = int(os.environ.get("MCP_PORT", 8001))

        def run_mcp():
            print(f"ğŸš€ MCP Server ì‹œì‘ â€” Port {mcp_port}")
            mcp.run(transport="sse", port=mcp_port)

        mcp_thread = threading.Thread(target=run_mcp, daemon=True)
        mcp_thread.start()

        print(f"ğŸš€ ì¹´ì¹´ì˜¤ ì›¹í›… Server ì‹œì‘ â€” Port {port}")
        uvicorn.run(app, host="0.0.0.0", port=port)

    else:
        # ì¹´ì¹´ì˜¤ ì›¹í›… ì „ìš© ëª¨ë“œ (ê¸°ë³¸)
        print(f"ğŸš€ DMF ì¹´ì¹´ì˜¤ ì±—ë´‡ Server ì‹œì‘ â€” Port {port}")
        print(f"   ì›¹í›… URL: https://YOUR-APP.onrender.com/kakao/skill")
        uvicorn.run(app, host="0.0.0.0", port=port)

